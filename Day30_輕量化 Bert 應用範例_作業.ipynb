{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業 : 微調輕量化 Bert 預訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [作業目標]\n",
    "- 觀察切換 distilBERT 及 Bert 模型帶來的影響\n",
    "- 嘗試並了解前處理對輕量化 Bert 模型帶來的影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [作業重點]\n",
    "- 試著替換不同的預訓練模型 (DistilBERT / Bert)，觀察有何不同\n",
    "(Hint : 在 In[7] 可以用註解切換不同模型) \n",
    "- 試著註解或跳過\"前處理\"的3個步驟，觀察有何不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件, 第一次執行前需安裝 transformers 套件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# scipy version == 1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入訓練與測試資料\n",
    "df = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "df_test = pd.read_csv('data/nlp-getting-started/test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前處理-1 : 消除連字\n",
    "def decontracted(text):\n",
    "    # 特殊連字\n",
    "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
    "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
    "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
    "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
    "    # 一般性連字\n",
    "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
    "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
    "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: decontracted(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: decontracted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前處理-2 : 清除特殊符號\n",
    "import string\n",
    "regular_punct = list(string.punctuation)\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "# 消除標點符號以及上列符號\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "# 消除連字號 \"-\" 以及句號 \".\"\n",
    "all_punct.remove('-')\n",
    "all_punct.remove('.')\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: spacing_punctuation(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: spacing_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前處理-3 : 錯漏字修正\n",
    "mis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\n",
    "mis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n",
    "\n",
    "mis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n",
    "                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n",
    "                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n",
    "                      # why\n",
    "                      'Whybis':'Why is', 'laowhy86':'Foreigners who do not respect China',\n",
    "                      'Whyco-education':'Why co-education',\n",
    "                      # How\n",
    "                      \"Howddo\":\"How do\", 'Howeber':'However', 'Showh':'Show',\n",
    "                      \"Willowmagic\":'Willow magic', 'WillsEye':'Will Eye', 'Williby':'will by'}\n",
    "def spacing_some_connect_words(text):\n",
    "    \"\"\"\n",
    "    'Whyare' -> 'Why are'\n",
    "    \"\"\"\n",
    "    ori = text\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])\n",
    "            \n",
    "    # what\n",
    "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    # why\n",
    "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    # How\n",
    "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    # which\n",
    "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    # where\n",
    "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    \n",
    "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
    "    text = text.replace(\"What sApp\", 'WhatsApp') \n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: spacing_some_connect_words(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: spacing_some_connect_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 , 000 people receive  # wildfires evacuatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this  # earthquake...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to  ' shelter in place '  ...   \n",
       "3   6     NaN      NaN  13 , 000 people receive  # wildfires evacuatio...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby  # Alaska a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入 distilBERT 模型或 Bert 模型, 將文字編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 distilBERT 模型或 Bert 模型 (下列兩行中, 將不選的模型註解掉即可)\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# 載入預訓練權重以及 tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整訓練資料的大小 (可取消, 若不取消表示取前4000筆訓練)\n",
    "df = df[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將訓練資料經由 distilBERT 或 Bert 轉換為 Embedding 編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將訓練資料經過 tokenizer 編碼轉換\n",
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以最長字串為準, 將訓練資料補零成相同長度\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 attention_mask, 將計算經過 Bert 生成的 Embedding 結果, 儲存於 last_hidden_states 中\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "input_ids = torch.tensor(padded).to(torch.int64)\n",
    "attention_mask = torch.tensor(attention_mask).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 準備下一階段要用的特徵 (上階段 Embedding 結果) 與目標值\n",
    "labels = df['target']\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割訓練 / 測試集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Logistic Regression 當作最後一層, 輸出預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 5.263252631578947}\n",
      "best scrores:  0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "# 對 Logistic Regression 跑參, 相當於加上單層類神經網路\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將上一格跑出的 Logistic Regression 最佳 C 值填入, 觀察測試集的驗證分數\n",
    "lr_clf = LogisticRegression(C = 5.263252631578947)  \n",
    "lr_clf.fit(train_features, train_labels)\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 對預測目標資料做出最終預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將預測目標資料經過 tokenizer 編碼轉換\n",
    "tokenized_t = df_test['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 73)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以最長字串為準, 將預測目標資料補零成相同長度\n",
    "max_len = 0\n",
    "for i in tokenized_t.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "padded_t = np.array([i + [0]*(max_len-len(i)) for i in tokenized_t.values])\n",
    "np.array(padded_t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 attention_mask, 將計算經過 Bert 生成的 Embedding 結果, 儲存於 last_hidden_states 中\n",
    "attention_mask_t = np.where(padded_t != 0, 1, 0)\n",
    "input_ids = torch.tensor(padded_t).to(torch.int64)\n",
    "attention_mask_t = torch.tensor(attention_mask_t).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸出預測目標資料的預測結果\n",
    "val_features = last_hidden_states[0][:,0,:].numpy() \n",
    "y_pred = lr_clf.predict(val_features)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成提交擋\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['target'] = y_pred\n",
    "submission.to_csv('submission_DistilBert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
