{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Pf_RxOIAYv"
   },
   "source": [
    "### 作業目的: 透過實作加速版word2vec Skip-gram模型來更加了解高速版的word2vec\n",
    "\n",
    "本次作業會採用Penn Tree Bank資料及，學員可以在ptb.train.txt中取得訓練文本資料。這次作業可以讓學員練習到以pytorch搭建模型與進行文本資料的前處理\n",
    "\n",
    "PS: 建議學員使用Colab (或可以使用GPU加速的機器)來進行作業，不然訓練會訓練到天荒地老....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO-a6e2OI5zg"
   },
   "source": [
    "### Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14937,
     "status": "ok",
     "timestamp": 1606320756664,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "LXPU7BI3HNJ6"
   },
   "outputs": [],
   "source": [
    "# # Import libraries for importing files from Google drive to Colab\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authorize Google SDK to access Google Drive from Colab\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17072,
     "status": "ok",
     "timestamp": 1606320758810,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "D2E7yb-qI9Uv"
   },
   "outputs": [],
   "source": [
    "# download = drive.CreateFile({'id': '請自行輸入自己上傳google drive檔案的連結id'})\n",
    "# download.GetContentFile('ptb.train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKKpFV6GJwhs"
   },
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4149,
     "status": "ok",
     "timestamp": 1606320764926,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Yjz-fWmbJRPB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1606320764930,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "i9xrgPu3KBgJ",
    "outputId": "341dcbac-256c-45ee-ed8d-3ee0c1fb03b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 42068 lines\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "\n",
    "# Penn Tree Back dataset\n",
    "with open(\"./ptb.train.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"Total {len(lines)} lines\")\n",
    "raw_dataset = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2988,
     "status": "ok",
     "timestamp": 1606320764931,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "oAcF_5CQKH_J",
    "outputId": "b42ef993-9894-4061-f8df-3f929fe17114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aer',\n",
       "  'banknote',\n",
       "  'berlitz',\n",
       "  'calloway',\n",
       "  'centrust',\n",
       "  'cluett',\n",
       "  'fromstein',\n",
       "  'gitano',\n",
       "  'guterman',\n",
       "  'hydro-quebec',\n",
       "  'ipo',\n",
       "  'kia',\n",
       "  'memotec',\n",
       "  'mlx',\n",
       "  'nahb',\n",
       "  'punts',\n",
       "  'rake',\n",
       "  'regatta',\n",
       "  'rubens',\n",
       "  'sim',\n",
       "  'snack-food',\n",
       "  'ssangyong',\n",
       "  'swapo',\n",
       "  'wachter'],\n",
       " ['pierre',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'will',\n",
       "  'join',\n",
       "  'the',\n",
       "  'board',\n",
       "  'as',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'nov.',\n",
       "  'N'],\n",
       " ['mr.',\n",
       "  '<unk>',\n",
       "  'is',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'n.v.',\n",
       "  'the',\n",
       "  'dutch',\n",
       "  'publishing',\n",
       "  'group'],\n",
       " ['rudolph',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'of',\n",
       "  'this',\n",
       "  'british',\n",
       "  'industrial',\n",
       "  'conglomerate'],\n",
       " ['a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  'has',\n",
       "  'caused',\n",
       "  'a',\n",
       "  'high',\n",
       "  'percentage',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'deaths',\n",
       "  'among',\n",
       "  'a',\n",
       "  'group',\n",
       "  'of',\n",
       "  'workers',\n",
       "  'exposed',\n",
       "  'to',\n",
       "  'it',\n",
       "  'more',\n",
       "  'than',\n",
       "  'N',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'researchers',\n",
       "  'reported']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5筆\n",
    "raw_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1606320765980,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "3oki6AxhJyj4",
    "outputId": "79a06d23-2176-4600-b55b-033dfb81ce49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling: 885720 words\n",
      "After subsampling: 449032 words\n"
     ]
    }
   ],
   "source": [
    "# 定義資料前處理函示\n",
    "class PreProcessor():\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    min_freq: int\n",
    "        minimum frequency of a word to be kept\n",
    "    do_subsampling: bool\n",
    "        whether to do subsampling\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, only_word: bool=False, min_freq: int=5, do_subsampling: bool=True, t: float=1e-5):\n",
    "        self.only_word = only_word\n",
    "        self.min_freq = min_freq\n",
    "        self.do_subsampling = do_subsampling\n",
    "        self.t = t\n",
    "    \n",
    "    def process(self, corpus: List[str]):\n",
    "        \n",
    "        word_dic = set()\n",
    "        counter = Counter()\n",
    "        processed_sentence = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "        \n",
    "            # hint: 請計算字詞頻率\n",
    "            counter.update(sentence)\n",
    "            processed_sentence.append(sentence)\n",
    "    \n",
    "        # hint: 移除頻率過小的字詞 建立word2idx與idx2word與word_frequency辭典\n",
    "        word_cnt = dict(filter(lambda x:x[1]>self.min_freq, counter.items()))\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(word_cnt.keys())}\n",
    "        self.idx2word ={idx: word for word, idx in self.word2idx.items()}\n",
    "        self.word_frequency = word_cnt.copy()\n",
    "        \n",
    "        #將文本轉為ID型式與移除文本中頻率過小的文字\n",
    "        self.processed_corpus = [[self.word2idx[word] for word in line if word in self.word2idx] for line in processed_sentence]\n",
    "        self.total_num_words = sum([len(line) for line in self.processed_corpus])\n",
    "        print(f\"Before subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        # 進行二次採樣(subsampling)\n",
    "        if self.do_subsampling:\n",
    "            self.processed_corpus = [[idx for idx in line if not self.subsampling(idx)] for line in self.processed_corpus]\n",
    "            self.total_num_words = sum([len(line) for line in self.processed_corpus])\n",
    "            counter = Counter([self.idx2word[idx] for line in self.processed_corpus for idx in line])\n",
    "            self.word_frequency = dict(counter.items())\n",
    "            print(f\"After subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        # hint: 移除空字串\n",
    "        self.processed_corpus = [line for line in self.processed_corpus if len(line) != 0]\n",
    "        \n",
    "        return self.processed_corpus, self.word2idx, self.idx2word, self.word_frequency, self.total_num_words\n",
    "\n",
    "    def subsampling(self, idx):\n",
    "        \n",
    "        # hint: 學員可以參考講義的subsampling公式(也可自己定義一個)\n",
    "        \n",
    "        p = self.t / self.word_frequency[self.idx2word[idx]] * self.total_num_words\n",
    "        p_w = math.sqrt(p) + p\n",
    "        return random.uniform(0, 1) < p_w\n",
    "\n",
    "\n",
    "# 進行資料前處理\n",
    "# 這邊我們subsampling的t取1e-4\n",
    "pre_processor = PreProcessor(True, 5, True, 1e-4)\n",
    "corpus, word2idx, idx2word, word2freq, total_num_words = pre_processor.process(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfDuJuT5Kkvl"
   },
   "source": [
    "### 定義Skip-gram使用的Dataset與collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 客製化Dataset\n",
    "class SkipGramGetAllDataset(Dataset):\n",
    "    def __init__(self, corpus, word2freq, word2idx, idx2word, window_size, num_negatives):\n",
    "        self.corpus = corpus\n",
    "        self.word2freq = word2freq\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.window_size = window_size\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        self.all_targets, self.all_contexts = self._get_all_contexts_targets()\n",
    "        self.all_negatives = self._get_all_negatives()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # hint: 這裡我們會返回 目標字詞，上下文，負採樣樣本\n",
    "        return (self.all_targets[idx], self.all_contexts[idx], self.all_negatives[idx])\n",
    "        \n",
    "    def _get_all_contexts_targets(self):\n",
    "        all_targets = []\n",
    "        all_contexts = []\n",
    "        \n",
    "        for line in self.corpus:\n",
    "            if len(line) < 2 * self.window_size + 1:\n",
    "                continue\n",
    "            \n",
    "            # hint: 這邊我們要創建上下文(考慮window_size)\n",
    "            all_contexts += line[self.window_size:-self.window_size]\n",
    "\n",
    "            targets = []\n",
    "            for idx in range(self.window_size, len(line) - self.window_size):\n",
    "                # hint: 創建目標字詞\n",
    "                indices = list(range(idx - self.window_size, idx + self.window_size + 1))\n",
    "                indices.remove(idx)\n",
    "                all_targets.append([line[i] for i in indices])\n",
    "\n",
    "        return all_targets, all_contexts\n",
    "                               \n",
    "    \n",
    "    def _get_all_negatives(self):\n",
    "        # hint: 進行負採樣，若沒頭緒的學員可以參考實作範例\n",
    "        cur_exists_words = list(self.word2freq.keys())\n",
    "        sampling_weights = [self.word2freq[word] ** 0.75 for word in self.word2freq]\n",
    "        population = list(range(len(sampling_weights)))\n",
    "        \n",
    "        all_negatives = []\n",
    "        for targets in self.all_targets:\n",
    "            negatives = []\n",
    "            while len(negatives) < self.num_negatives:\n",
    "                neg_candidate = random.choices(population, sampling_weights)[0]\n",
    "                neg_cnadidate = self.word2idx[cur_exists_words[neg_candidate]]\n",
    "                if neg_candidate not in targets:\n",
    "                    negatives.append(neg_candidate)\n",
    "            all_negatives.append(negatives)\n",
    "        \n",
    "        return all_negatives\n",
    "    \n",
    "# 客製化collate_fn\n",
    "def skipgram_collate(data):\n",
    "    contexts = []\n",
    "    target_negative = []\n",
    "    labels = []\n",
    "    for target, context, negative in data:\n",
    "        # hint: 將目標字詞、上下文與負採樣樣本個別打包\n",
    "        target_negative += [target + negative]\n",
    "        labels += [[1] * len(target) + [0] * len(negative)]\n",
    "        contexts += [context]\n",
    "    \n",
    "    return torch.tensor(contexts), torch.tensor(target_negative), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s94kJ0lKKzG5"
   },
   "source": [
    "### 定義Skip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1606320766292,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "kyyQyLxcKpv1"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, contexts, targets):\n",
    "        v = self.in_embedding(contexts)\n",
    "        u = self.out_embedding(targets)\n",
    "        \n",
    "        # do dot product to get output\n",
    "        pred = torch.matmul(v.unsqueeze(1), u.permute(0,2,1))\n",
    "        \n",
    "        return pred.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHZIFz7yK5An"
   },
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13745,
     "status": "ok",
     "timestamp": 1606320780465,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Hr4sVBd8K10T"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "verbose = True\n",
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "embed_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "model = SkipGram(len(word2idx), embed_size)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataset = SkipGramGetAllDataset(corpus, word2freq, word2idx, idx2word, 1, 5)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=skipgram_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219482,
     "status": "ok",
     "timestamp": 1606321001876,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "sE28LW2_LB0I",
    "outputId": "4cf7c434-b5b8-4e73-eb3c-c2735ef0d17c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Batch: 501/715.5703125 Loss: 0.70977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:22<07:02, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Loss: 0.58289\n",
      "Epoch: 2/20, Batch: 501/715.5703125 Loss: 0.25806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:43<06:37, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20, Loss: 0.25582\n",
      "Epoch: 3/20, Batch: 501/715.5703125 Loss: 0.24234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [01:05<06:12, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20, Loss: 0.24239\n",
      "Epoch: 4/20, Batch: 501/715.5703125 Loss: 0.23741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [01:26<05:48, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20, Loss: 0.23769\n",
      "Epoch: 5/20, Batch: 501/715.5703125 Loss: 0.23449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:48<05:25, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20, Loss: 0.23499\n",
      "Epoch: 6/20, Batch: 501/715.5703125 Loss: 0.23264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [02:09<05:03, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20, Loss: 0.23305\n",
      "Epoch: 7/20, Batch: 501/715.5703125 Loss: 0.23142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [02:31<04:40, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20, Loss: 0.23167\n",
      "Epoch: 8/20, Batch: 501/715.5703125 Loss: 0.22981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [02:53<04:20, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20, Loss: 0.23069\n",
      "Epoch: 9/20, Batch: 501/715.5703125 Loss: 0.22961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [03:15<03:58, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20, Loss: 0.23001\n",
      "Epoch: 10/20, Batch: 501/715.5703125 Loss: 0.22882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [03:36<03:36, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20, Loss: 0.22947\n",
      "Epoch: 11/20, Batch: 501/715.5703125 Loss: 0.22825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [03:58<03:15, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20, Loss: 0.22905\n",
      "Epoch: 12/20, Batch: 501/715.5703125 Loss: 0.22817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [04:20<02:54, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20, Loss: 0.22885\n",
      "Epoch: 13/20, Batch: 501/715.5703125 Loss: 0.22820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [04:42<02:32, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20, Loss: 0.22867\n",
      "Epoch: 14/20, Batch: 501/715.5703125 Loss: 0.22799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [05:04<02:11, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20, Loss: 0.22843\n",
      "Epoch: 15/20, Batch: 501/715.5703125 Loss: 0.22767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [05:25<01:48, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20, Loss: 0.22842\n",
      "Epoch: 16/20, Batch: 501/715.5703125 Loss: 0.22774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [05:47<01:26, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20, Loss: 0.22824\n",
      "Epoch: 17/20, Batch: 501/715.5703125 Loss: 0.22771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [06:09<01:05, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20, Loss: 0.22819\n",
      "Epoch: 18/20, Batch: 501/715.5703125 Loss: 0.22765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [06:30<00:43, 21.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20, Loss: 0.22810\n",
      "Epoch: 19/20, Batch: 501/715.5703125 Loss: 0.22771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [06:52<00:21, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20, Loss: 0.22799\n",
      "Epoch: 20/20, Batch: 501/715.5703125 Loss: 0.22756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:14<00:00, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20, Loss: 0.22798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "lst_loss = []\n",
    "model.train()\n",
    "for epc in tqdm.tqdm(range(num_epochs)):\n",
    "    batch_loss = 0\n",
    "\n",
    "    for i, (contexts, target_negative, labels) in enumerate(loader, 1):\n",
    "        # hint: 開始訓練前要先將optimizer的梯度歸零\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            contexts = contexts.cuda()\n",
    "            target_negative = target_negative.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        pred = model(contexts, target_negative)\n",
    "        loss = criterion(pred, labels.float())\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch: {epc + 1}/{num_epochs}, Batch: {i+1}/{len(dataset)/batch_size} Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Epoch: {epc + 1}/{num_epochs}, Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    lst_loss.append(batch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1606321013487,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "y0rt5W2ELLvP",
    "outputId": "b497edcc-fc8e-47d3-b3ff-c574d42ff581"
   },
   "outputs": [],
   "source": [
    "# visualization loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lst_loss, marker='s')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Word2Vec Skip-gram Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1606321030038,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "43pOYRh-MX_F",
    "outputId": "3de0675b-47af-462d-c927-f14a62933956"
   },
   "outputs": [],
   "source": [
    "#計算字詞相似度\n",
    "\n",
    "def get_similarity(word, top_k, model, word2idx, idx2word):\n",
    "    W = (model.in_embedding.weight.data + model.out_embedding.weight.data) / 2\n",
    "    idx = word2idx.get(word, None)\n",
    "    \n",
    "    if not idx:\n",
    "        # 當出現不在字典中的字詞時，顯示Out of vocabulary error\n",
    "        raise ValueError(\"Out of vocabulary\")\n",
    "    else:\n",
    "        x = W[idx]\n",
    "        \n",
    "        # 使用cosine相似計算字詞間的相似程度\n",
    "        cos = torch.matmul(W, x) / (torch.sum(W * W, dim=-1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "        _, topk = torch.topk(cos, top_k+1)\n",
    "        \n",
    "        for i in topk[1:]:\n",
    "            print(f\"cosine sim={cos[int(i)]:.3f}: {idx2word[int(i)]}.\")\n",
    "\n",
    "get_similarity('love', 4, model, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_tL9g0oMcCT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNRbZbSHSpiMTWmQCCagSqg",
   "collapsed_sections": [],
   "name": "word2vec高速化_作業解答.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
