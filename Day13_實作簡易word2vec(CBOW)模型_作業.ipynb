{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 透過實作深入了解word2vec模型\n",
    "\n",
    "在前兩次的課程中，我們了解的word2vec的兩種模型(CBOW/Skip-gram)，接下來來看看如何實際搭建word2vec的模型。 本次課程講解會使用CBOW模型作為例子來進行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建基礎層\n",
    "\n",
    "由前面的課程可以學到，訓練word2vec模型時採用的神經網路為全連接層(FC)。在輸出層的部分則是採用softmax，損失函數使用cross entropy。\n",
    "\n",
    "FC層的核心計算就是矩陣向量乘法。因此在開始搭建CBOW模型前，需要先搭建基本的FC層(矩陣乘法)與欲使用的softmax與cross entropy函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FC/Dense層: 損失函數對輸入的偏微分如下 (此部分計算有興趣的同學可以參考Appendix，本章節重點在於實作word2vec模型)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{L}}{\\partial{x}} &= \\sum_j\\frac{\\partial{L}}{\\partial{y}}W^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} &= X^T \\sum_j\\frac{\\partial{L}}{\\partial{y}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "2. Softmax & Loss層: 在訓練word2vec主要使用的輸出層函數為softmax，而使用的損失函數為cross entropy，其中偏微分如下\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{SoftmaxIn}} = SoftmaxOut - \\hat{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from utils.utility import clip_grads, remove_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax function\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = np.exp(x)\n",
    "        x = np.divide(x, x.sum(axis=1, keepdims=True) + 1e-7)\n",
    "    elif x.ndim == 1:\n",
    "        x = np.exp(x)\n",
    "        x = np.divide(x, np.sum(x) + 1e-7)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# define cross entropy\n",
    "def cross_entropy(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -(1/batch_size) * np.sum(np.log(y[np.arange(batch_size), t] + 1e-7))\n",
    "\n",
    "# Define FC layer\n",
    "class Dense():\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w = self.params[0]\n",
    "        out = np.dot(x, w)\n",
    "        self.x = x\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        w = self.params[0]\n",
    "        dx = np.dot(dout, w.T) #dx = dy * W^T\n",
    "        dw = np.dot(self.x.T, dout) #dw = x^T * dx\n",
    "        self.grads[0][...] = dw\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# define softmax with cross entropy layer\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        \n",
    "        loss = cross_entropy(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        dx = self.y.copy() #softmax output\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx /= batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "\n",
    "除了建構搭建模型的基礎元素外，還需要將輸入的文本資料進行前置處理:\n",
    "\n",
    "1. Tokenize: 將文本的字詞轉換為index\n",
    "2. Data label pair: 將文本資料轉換為訓練資料與目標字詞的配對\n",
    "3. One-hot encoding: 將訓練資料與目標字詞轉換為one-hot的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define preprocess function\n",
    "def preprocess(corpus: List[str], only_word: bool = False):\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    '''\n",
    "    word_dic = set()\n",
    "    processed_sentence = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        #將所有字詞轉為小寫\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        #移除標點符號(可以依據使用狀況決定是否要移除標點符號)\n",
    "        if only_word:\n",
    "            pattern = r'[^\\W_]+'\n",
    "            sentence = re.findall(pattern, sentence)\n",
    "        else:\n",
    "            punctuation_list = ['.', ',', '!', '?']\n",
    "            for pun in punctuation_list:\n",
    "                sentence = sentence.replace(pun, ' '+pun)\n",
    "            sentence = sentence.split(' ')\n",
    "        \n",
    "        #添加字詞到字典中\n",
    "        word_dic |= set(sentence)\n",
    "        processed_sentence.append(sentence)\n",
    "    \n",
    "    \n",
    "    #建立字詞ID清單\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for word in word_dic:\n",
    "        if word not in word2idx:\n",
    "            idx = len(word2idx)\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "\n",
    "    #將文本轉為ID型式\n",
    "    id_mapping = lambda x: word2idx[x]\n",
    "    \n",
    "    corpus = np.array([list(map(id_mapping, sentence)) for sentence in processed_sentence])\n",
    "\n",
    "    return corpus, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 5, 2, 0, 4, 6, 3, 7]]),\n",
       " {'natural': 0,\n",
       "  'i': 1,\n",
       "  'studying': 2,\n",
       "  'now': 3,\n",
       "  'language': 4,\n",
       "  'am': 5,\n",
       "  'processing': 6,\n",
       "  '.': 7},\n",
       " {0: 'natural',\n",
       "  1: 'i',\n",
       "  2: 'studying',\n",
       "  3: 'now',\n",
       "  4: 'language',\n",
       "  5: 'am',\n",
       "  6: 'processing',\n",
       "  7: '.'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test preprocessing\n",
    "text = \"I am studying Natural Language Processing now.\"\n",
    "corpus, word2idx, idx2word = preprocess([text])\n",
    "corpus, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create contexts - target\n",
    "def create_contexts_target(corpus: List, window_size: int=1):\n",
    "\n",
    "    contexts = []\n",
    "    targets = corpus[window_size:-window_size]\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                # skip target word itself\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 4, 0, 5],\n",
       "        [4, 1, 5, 4],\n",
       "        [1, 0, 4, 3],\n",
       "        [0, 5, 3, 6]]),\n",
       " array([1, 0, 5, 4]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test contexts target function\n",
    "contexts, targets= create_contexts_target(corpus[0], window_size=2)\n",
    "contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0]],\n",
       " \n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0]],\n",
       " \n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]]], dtype=int32),\n",
       " array([[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0]], dtype=int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = convert_one_hot(contexts, len(word2idx))\n",
    "targets = convert_one_hot(targets, len(word2idx))\n",
    "contexts, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建CBOW模型\n",
    "\n",
    "在完成所以的前置作業(建構基本元素、資料前處理)後，可以開始搭建word2vec CBOW模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # initialize weights\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # create layers\n",
    "        self.in_layers = [Dense(W_in) for i in range(window_size*2)]\n",
    "        self.out_layer = Dense(W_out)\n",
    "        self.loss_layer = SoftmaxWithCrossEntropy()\n",
    "\n",
    "\n",
    "        layers = self.in_layers + [self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        hs = sum([self.in_layers[i].forward(contexts[:, i]) for i in range(self.window_size*2)])\n",
    "        hs /= self.window_size*2\n",
    "        \n",
    "        score = self.out_layer.forward(hs)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da /= self.window_size*2\n",
    "        \n",
    "        for i in range(self.window_size*2):\n",
    "            self.in_layers[i].backward(da)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer與Optimizer\n",
    "\n",
    "搭建完模型後，最後一個步驟就是需要一個optimizer來針對由loss取得的gradient來更新模型的參數，optimizer像是模型參數的更新策略，這邊因為重點不在optimizer，因此會使用最單純好懂的SGD(Stochastic Gradient Descent)來當作optimizer\n",
    "\n",
    "$$\n",
    "W_t = W_{t-1} - \\alpha\\frac{\\partial{L}}{\\partial{W_{t-1}}}\n",
    "$$\n",
    "\n",
    "當所有都準備就緒後，就是串起資料、模型、optimizer來開始進行訓練了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "class SGD:\n",
    "    '''\n",
    "    Stochastic Gradient Descent\n",
    "    '''\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "            \n",
    "# define trainer for training purpose\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(max_epoch)):\n",
    "            # shuffling\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # calculate loss and update weights\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                # remove duplicate weights (for weights sharing purpose)\n",
    "                params, grads = remove_duplicate(model.params, model.grads) \n",
    "                # for gradient clipping purpose\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                    \n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # evaluation\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    print(f\"Epoch: {self.current_epoch+1}, Iteration: {iters+1}/{max_iters}, Loss: {avg_loss}\")\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = list(range(len(self.loss_list)))\n",
    "        \n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel(f\"iterations (x{self.eval_interval})\")\n",
    "        plt.ylabel(f\"loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練\n",
    "\n",
    "一切準備就緒，可以開始訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 213/1000 [00:00<00:00, 1091.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1/2, Loss: 1.94587569777337\n",
      "Epoch: 2, Iteration: 1/2, Loss: 1.9458634437889968\n",
      "Epoch: 3, Iteration: 1/2, Loss: 1.9458644212644094\n",
      "Epoch: 4, Iteration: 1/2, Loss: 1.945787822634224\n",
      "Epoch: 5, Iteration: 1/2, Loss: 1.9458148055108373\n",
      "Epoch: 6, Iteration: 1/2, Loss: 1.9458757259902397\n",
      "Epoch: 7, Iteration: 1/2, Loss: 1.945781226508115\n",
      "Epoch: 8, Iteration: 1/2, Loss: 1.9457510597188101\n",
      "Epoch: 9, Iteration: 1/2, Loss: 1.9457687610103194\n",
      "Epoch: 10, Iteration: 1/2, Loss: 1.9457487919567784\n",
      "Epoch: 11, Iteration: 1/2, Loss: 1.9457141526898551\n",
      "Epoch: 12, Iteration: 1/2, Loss: 1.9457167087021072\n",
      "Epoch: 13, Iteration: 1/2, Loss: 1.9457527965384773\n",
      "Epoch: 14, Iteration: 1/2, Loss: 1.9456528862708589\n",
      "Epoch: 15, Iteration: 1/2, Loss: 1.9456273641999684\n",
      "Epoch: 16, Iteration: 1/2, Loss: 1.9456213991925175\n",
      "Epoch: 17, Iteration: 1/2, Loss: 1.9456198421823205\n",
      "Epoch: 18, Iteration: 1/2, Loss: 1.9456933363648792\n",
      "Epoch: 19, Iteration: 1/2, Loss: 1.9455731330975745\n",
      "Epoch: 20, Iteration: 1/2, Loss: 1.945430079657441\n",
      "Epoch: 21, Iteration: 1/2, Loss: 1.9455579503251916\n",
      "Epoch: 22, Iteration: 1/2, Loss: 1.945625786424416\n",
      "Epoch: 23, Iteration: 1/2, Loss: 1.9453214631176015\n",
      "Epoch: 24, Iteration: 1/2, Loss: 1.9454426376786516\n",
      "Epoch: 25, Iteration: 1/2, Loss: 1.9453981768208768\n",
      "Epoch: 26, Iteration: 1/2, Loss: 1.945309043329935\n",
      "Epoch: 27, Iteration: 1/2, Loss: 1.9454078998673192\n",
      "Epoch: 28, Iteration: 1/2, Loss: 1.945241987115125\n",
      "Epoch: 29, Iteration: 1/2, Loss: 1.9454240716662428\n",
      "Epoch: 30, Iteration: 1/2, Loss: 1.9451003702438883\n",
      "Epoch: 31, Iteration: 1/2, Loss: 1.9450708180157208\n",
      "Epoch: 32, Iteration: 1/2, Loss: 1.9452837422170006\n",
      "Epoch: 33, Iteration: 1/2, Loss: 1.9449022485237046\n",
      "Epoch: 34, Iteration: 1/2, Loss: 1.9449673674666892\n",
      "Epoch: 35, Iteration: 1/2, Loss: 1.9448538853835102\n",
      "Epoch: 36, Iteration: 1/2, Loss: 1.9450857202802818\n",
      "Epoch: 37, Iteration: 1/2, Loss: 1.9445245469983106\n",
      "Epoch: 38, Iteration: 1/2, Loss: 1.9449164014877862\n",
      "Epoch: 39, Iteration: 1/2, Loss: 1.944673735123965\n",
      "Epoch: 40, Iteration: 1/2, Loss: 1.944354210927667\n",
      "Epoch: 41, Iteration: 1/2, Loss: 1.9443799352477764\n",
      "Epoch: 42, Iteration: 1/2, Loss: 1.9442484938909215\n",
      "Epoch: 43, Iteration: 1/2, Loss: 1.9442191462818492\n",
      "Epoch: 44, Iteration: 1/2, Loss: 1.9444152091365827\n",
      "Epoch: 45, Iteration: 1/2, Loss: 1.943970850792183\n",
      "Epoch: 46, Iteration: 1/2, Loss: 1.9435873254324285\n",
      "Epoch: 47, Iteration: 1/2, Loss: 1.9439340913998404\n",
      "Epoch: 48, Iteration: 1/2, Loss: 1.943194967943682\n",
      "Epoch: 49, Iteration: 1/2, Loss: 1.9436051663176215\n",
      "Epoch: 50, Iteration: 1/2, Loss: 1.9430807582556278\n",
      "Epoch: 51, Iteration: 1/2, Loss: 1.943019300371383\n",
      "Epoch: 52, Iteration: 1/2, Loss: 1.9431943693326361\n",
      "Epoch: 53, Iteration: 1/2, Loss: 1.942613173036565\n",
      "Epoch: 54, Iteration: 1/2, Loss: 1.9424918680815089\n",
      "Epoch: 55, Iteration: 1/2, Loss: 1.942158348081952\n",
      "Epoch: 56, Iteration: 1/2, Loss: 1.9418788870024846\n",
      "Epoch: 57, Iteration: 1/2, Loss: 1.9409878110196606\n",
      "Epoch: 58, Iteration: 1/2, Loss: 1.9412562921769687\n",
      "Epoch: 59, Iteration: 1/2, Loss: 1.9414591064018696\n",
      "Epoch: 60, Iteration: 1/2, Loss: 1.9405493412524106\n",
      "Epoch: 61, Iteration: 1/2, Loss: 1.940074505770133\n",
      "Epoch: 62, Iteration: 1/2, Loss: 1.9397978730345526\n",
      "Epoch: 63, Iteration: 1/2, Loss: 1.9390163635165867\n",
      "Epoch: 64, Iteration: 1/2, Loss: 1.9386549768504944\n",
      "Epoch: 65, Iteration: 1/2, Loss: 1.9387618320662527\n",
      "Epoch: 66, Iteration: 1/2, Loss: 1.938422574279898\n",
      "Epoch: 67, Iteration: 1/2, Loss: 1.9368783800078653\n",
      "Epoch: 68, Iteration: 1/2, Loss: 1.9366738226136573\n",
      "Epoch: 69, Iteration: 1/2, Loss: 1.9358036619644168\n",
      "Epoch: 70, Iteration: 1/2, Loss: 1.9362053104500918\n",
      "Epoch: 71, Iteration: 1/2, Loss: 1.9327487147641993\n",
      "Epoch: 72, Iteration: 1/2, Loss: 1.9346841761698943\n",
      "Epoch: 73, Iteration: 1/2, Loss: 1.9317604612177042\n",
      "Epoch: 74, Iteration: 1/2, Loss: 1.932829306005607\n",
      "Epoch: 75, Iteration: 1/2, Loss: 1.9305904953496358\n",
      "Epoch: 76, Iteration: 1/2, Loss: 1.9311588869873817\n",
      "Epoch: 77, Iteration: 1/2, Loss: 1.9270977956601865\n",
      "Epoch: 78, Iteration: 1/2, Loss: 1.926936863261132\n",
      "Epoch: 79, Iteration: 1/2, Loss: 1.926554177635688\n",
      "Epoch: 80, Iteration: 1/2, Loss: 1.9228721825273296\n",
      "Epoch: 81, Iteration: 1/2, Loss: 1.9226873377758702\n",
      "Epoch: 82, Iteration: 1/2, Loss: 1.9234035933074694\n",
      "Epoch: 83, Iteration: 1/2, Loss: 1.9192715325148457\n",
      "Epoch: 84, Iteration: 1/2, Loss: 1.9179720702697045\n",
      "Epoch: 85, Iteration: 1/2, Loss: 1.9166271982445502\n",
      "Epoch: 86, Iteration: 1/2, Loss: 1.9123313525513734\n",
      "Epoch: 87, Iteration: 1/2, Loss: 1.9124284215009637\n",
      "Epoch: 88, Iteration: 1/2, Loss: 1.9064526550176484\n",
      "Epoch: 89, Iteration: 1/2, Loss: 1.9063793864231442\n",
      "Epoch: 90, Iteration: 1/2, Loss: 1.9001234529878657\n",
      "Epoch: 91, Iteration: 1/2, Loss: 1.8997319704289122\n",
      "Epoch: 92, Iteration: 1/2, Loss: 1.8946482500780952\n",
      "Epoch: 93, Iteration: 1/2, Loss: 1.8991019653750314\n",
      "Epoch: 94, Iteration: 1/2, Loss: 1.8894440141966116\n",
      "Epoch: 95, Iteration: 1/2, Loss: 1.8838001111764509\n",
      "Epoch: 96, Iteration: 1/2, Loss: 1.8822933491678175\n",
      "Epoch: 97, Iteration: 1/2, Loss: 1.8714917986145956\n",
      "Epoch: 98, Iteration: 1/2, Loss: 1.8781196992400648\n",
      "Epoch: 99, Iteration: 1/2, Loss: 1.8631697608381952\n",
      "Epoch: 100, Iteration: 1/2, Loss: 1.86380870289296\n",
      "Epoch: 101, Iteration: 1/2, Loss: 1.84718495419359\n",
      "Epoch: 102, Iteration: 1/2, Loss: 1.8580981997213897\n",
      "Epoch: 103, Iteration: 1/2, Loss: 1.8525294490733537\n",
      "Epoch: 104, Iteration: 1/2, Loss: 1.8263961037756329\n",
      "Epoch: 105, Iteration: 1/2, Loss: 1.827643128171802\n",
      "Epoch: 106, Iteration: 1/2, Loss: 1.8154141459769808\n",
      "Epoch: 107, Iteration: 1/2, Loss: 1.8118300072394666\n",
      "Epoch: 108, Iteration: 1/2, Loss: 1.8097178071160047\n",
      "Epoch: 109, Iteration: 1/2, Loss: 1.794017770089231\n",
      "Epoch: 110, Iteration: 1/2, Loss: 1.7842662950261274\n",
      "Epoch: 111, Iteration: 1/2, Loss: 1.7575238310960684\n",
      "Epoch: 112, Iteration: 1/2, Loss: 1.7834790848761675\n",
      "Epoch: 113, Iteration: 1/2, Loss: 1.7428252188230409\n",
      "Epoch: 114, Iteration: 1/2, Loss: 1.7511822551983567\n",
      "Epoch: 115, Iteration: 1/2, Loss: 1.7069018467091572\n",
      "Epoch: 116, Iteration: 1/2, Loss: 1.713298239793594\n",
      "Epoch: 117, Iteration: 1/2, Loss: 1.7569532083752748\n",
      "Epoch: 118, Iteration: 1/2, Loss: 1.6645033468082548\n",
      "Epoch: 119, Iteration: 1/2, Loss: 1.6430697849819866\n",
      "Epoch: 120, Iteration: 1/2, Loss: 1.6945338525701725\n",
      "Epoch: 121, Iteration: 1/2, Loss: 1.680125517551196\n",
      "Epoch: 122, Iteration: 1/2, Loss: 1.597626678185227\n",
      "Epoch: 123, Iteration: 1/2, Loss: 1.5733952659696722\n",
      "Epoch: 124, Iteration: 1/2, Loss: 1.6440267639686392\n",
      "Epoch: 125, Iteration: 1/2, Loss: 1.623111628454935\n",
      "Epoch: 126, Iteration: 1/2, Loss: 1.5259402092106367\n",
      "Epoch: 127, Iteration: 1/2, Loss: 1.502206388044462\n",
      "Epoch: 128, Iteration: 1/2, Loss: 1.5789820929248073\n",
      "Epoch: 129, Iteration: 1/2, Loss: 1.551453239938478\n",
      "Epoch: 130, Iteration: 1/2, Loss: 1.4926954661068015\n",
      "Epoch: 131, Iteration: 1/2, Loss: 1.4945838524197192\n",
      "Epoch: 132, Iteration: 1/2, Loss: 1.4254168104103155\n",
      "Epoch: 133, Iteration: 1/2, Loss: 1.3740929469085517\n",
      "Epoch: 134, Iteration: 1/2, Loss: 1.4758667744736886\n",
      "Epoch: 135, Iteration: 1/2, Loss: 1.4390976281308387\n",
      "Epoch: 136, Iteration: 1/2, Loss: 1.3958897209061938\n",
      "Epoch: 137, Iteration: 1/2, Loss: 1.2954401997605218\n",
      "Epoch: 138, Iteration: 1/2, Loss: 1.4371570569425418\n",
      "Epoch: 139, Iteration: 1/2, Loss: 1.4243970842824818\n",
      "Epoch: 140, Iteration: 1/2, Loss: 1.2459370531657097\n",
      "Epoch: 141, Iteration: 1/2, Loss: 1.3180119330310838\n",
      "Epoch: 142, Iteration: 1/2, Loss: 1.3320171712462157\n",
      "Epoch: 143, Iteration: 1/2, Loss: 1.2626024902284043\n",
      "Epoch: 144, Iteration: 1/2, Loss: 1.2185270573894473\n",
      "Epoch: 145, Iteration: 1/2, Loss: 1.3523629889892081\n",
      "Epoch: 146, Iteration: 1/2, Loss: 1.1346943321080039\n",
      "Epoch: 147, Iteration: 1/2, Loss: 1.3499013979701466\n",
      "Epoch: 148, Iteration: 1/2, Loss: 1.200074723308599\n",
      "Epoch: 149, Iteration: 1/2, Loss: 1.2381409338169602\n",
      "Epoch: 150, Iteration: 1/2, Loss: 1.204416750177431\n",
      "Epoch: 151, Iteration: 1/2, Loss: 1.1708353323310274\n",
      "Epoch: 152, Iteration: 1/2, Loss: 1.1849984938719391\n",
      "Epoch: 153, Iteration: 1/2, Loss: 1.1977632391261093\n",
      "Epoch: 154, Iteration: 1/2, Loss: 1.0418556087904651\n",
      "Epoch: 155, Iteration: 1/2, Loss: 1.2592697939377793\n",
      "Epoch: 156, Iteration: 1/2, Loss: 1.1723853336203427\n",
      "Epoch: 157, Iteration: 1/2, Loss: 1.2134470719668777\n",
      "Epoch: 158, Iteration: 1/2, Loss: 0.9343968371736546\n",
      "Epoch: 159, Iteration: 1/2, Loss: 1.2614577749786318\n",
      "Epoch: 160, Iteration: 1/2, Loss: 1.082071257482744\n",
      "Epoch: 161, Iteration: 1/2, Loss: 1.013363291237299\n",
      "Epoch: 162, Iteration: 1/2, Loss: 1.3297554784950876\n",
      "Epoch: 163, Iteration: 1/2, Loss: 0.9944695772057839\n",
      "Epoch: 164, Iteration: 1/2, Loss: 1.058414685993335\n",
      "Epoch: 165, Iteration: 1/2, Loss: 1.1923502963720258\n",
      "Epoch: 166, Iteration: 1/2, Loss: 1.0053999590203504\n",
      "Epoch: 167, Iteration: 1/2, Loss: 1.0486551150788357\n",
      "Epoch: 168, Iteration: 1/2, Loss: 1.0670106380425575\n",
      "Epoch: 169, Iteration: 1/2, Loss: 1.060972369883205\n",
      "Epoch: 170, Iteration: 1/2, Loss: 1.0809035290089506\n",
      "Epoch: 171, Iteration: 1/2, Loss: 1.0516111355238384\n",
      "Epoch: 172, Iteration: 1/2, Loss: 1.1596173555310085\n",
      "Epoch: 173, Iteration: 1/2, Loss: 1.0495700256125804\n",
      "Epoch: 174, Iteration: 1/2, Loss: 0.7934403719711411\n",
      "Epoch: 175, Iteration: 1/2, Loss: 1.1784092067306347\n",
      "Epoch: 176, Iteration: 1/2, Loss: 0.9940805749317667\n",
      "Epoch: 177, Iteration: 1/2, Loss: 1.161600483562228\n",
      "Epoch: 178, Iteration: 1/2, Loss: 0.9233546041639658\n",
      "Epoch: 179, Iteration: 1/2, Loss: 0.8676119397296563\n",
      "Epoch: 180, Iteration: 1/2, Loss: 1.1695815727237182\n",
      "Epoch: 181, Iteration: 1/2, Loss: 1.0989563484850546\n",
      "Epoch: 182, Iteration: 1/2, Loss: 0.860581812752544\n",
      "Epoch: 183, Iteration: 1/2, Loss: 1.0120664952052252\n",
      "Epoch: 184, Iteration: 1/2, Loss: 1.0260919692916077\n",
      "Epoch: 185, Iteration: 1/2, Loss: 0.8958392608930197\n",
      "Epoch: 186, Iteration: 1/2, Loss: 0.9619261813841219\n",
      "Epoch: 187, Iteration: 1/2, Loss: 1.1077647493727092\n",
      "Epoch: 188, Iteration: 1/2, Loss: 0.9762456346881836\n",
      "Epoch: 189, Iteration: 1/2, Loss: 1.1293423373536204\n",
      "Epoch: 190, Iteration: 1/2, Loss: 0.8515723536807462\n",
      "Epoch: 191, Iteration: 1/2, Loss: 1.015024326178062\n",
      "Epoch: 192, Iteration: 1/2, Loss: 0.8252418329177136\n",
      "Epoch: 193, Iteration: 1/2, Loss: 1.0797925010873488\n",
      "Epoch: 194, Iteration: 1/2, Loss: 1.014656416375284\n",
      "Epoch: 195, Iteration: 1/2, Loss: 1.06403080397525\n",
      "Epoch: 196, Iteration: 1/2, Loss: 0.7619359954179283\n",
      "Epoch: 197, Iteration: 1/2, Loss: 1.0294786971444991\n",
      "Epoch: 198, Iteration: 1/2, Loss: 0.9669528554402512\n",
      "Epoch: 199, Iteration: 1/2, Loss: 0.9823606889517122\n",
      "Epoch: 200, Iteration: 1/2, Loss: 0.9260563437995782\n",
      "Epoch: 201, Iteration: 1/2, Loss: 0.9752333202635475\n",
      "Epoch: 202, Iteration: 1/2, Loss: 0.9227520435568555\n",
      "Epoch: 203, Iteration: 1/2, Loss: 0.970166396997737\n",
      "Epoch: 204, Iteration: 1/2, Loss: 0.8449192430724606\n",
      "Epoch: 205, Iteration: 1/2, Loss: 1.0525722839991154\n",
      "Epoch: 206, Iteration: 1/2, Loss: 0.7946748870470628\n",
      "Epoch: 207, Iteration: 1/2, Loss: 1.04976000605018\n",
      "Epoch: 208, Iteration: 1/2, Loss: 0.9323388377152867\n",
      "Epoch: 209, Iteration: 1/2, Loss: 0.9211796069742266\n",
      "Epoch: 210, Iteration: 1/2, Loss: 0.9587805088563999\n",
      "Epoch: 211, Iteration: 1/2, Loss: 0.8012741915446087\n",
      "Epoch: 212, Iteration: 1/2, Loss: 1.0102929911659237\n",
      "Epoch: 213, Iteration: 1/2, Loss: 0.8123379280701832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 396/1000 [00:00<00:00, 958.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214, Iteration: 1/2, Loss: 1.0557685775588883\n",
      "Epoch: 215, Iteration: 1/2, Loss: 0.792576221186855\n",
      "Epoch: 216, Iteration: 1/2, Loss: 0.8958697714631029\n",
      "Epoch: 217, Iteration: 1/2, Loss: 0.945387319004096\n",
      "Epoch: 218, Iteration: 1/2, Loss: 1.0831317623445644\n",
      "Epoch: 219, Iteration: 1/2, Loss: 0.8117566191386707\n",
      "Epoch: 220, Iteration: 1/2, Loss: 0.9017339632848277\n",
      "Epoch: 221, Iteration: 1/2, Loss: 1.0225624877697554\n",
      "Epoch: 222, Iteration: 1/2, Loss: 0.8977710395975572\n",
      "Epoch: 223, Iteration: 1/2, Loss: 0.8875510808858655\n",
      "Epoch: 224, Iteration: 1/2, Loss: 0.7935570161193626\n",
      "Epoch: 225, Iteration: 1/2, Loss: 0.7654220320685593\n",
      "Epoch: 226, Iteration: 1/2, Loss: 1.0185341960986904\n",
      "Epoch: 227, Iteration: 1/2, Loss: 0.7542213249129898\n",
      "Epoch: 228, Iteration: 1/2, Loss: 1.0059738555907485\n",
      "Epoch: 229, Iteration: 1/2, Loss: 0.8738858504060587\n",
      "Epoch: 230, Iteration: 1/2, Loss: 0.9532739809897697\n",
      "Epoch: 231, Iteration: 1/2, Loss: 0.7744298042896218\n",
      "Epoch: 232, Iteration: 1/2, Loss: 0.866942032746752\n",
      "Epoch: 233, Iteration: 1/2, Loss: 0.9760149937210902\n",
      "Epoch: 234, Iteration: 1/2, Loss: 0.669896978157974\n",
      "Epoch: 235, Iteration: 1/2, Loss: 1.0553148350663044\n",
      "Epoch: 236, Iteration: 1/2, Loss: 0.7444430399376605\n",
      "Epoch: 237, Iteration: 1/2, Loss: 0.9475480789793356\n",
      "Epoch: 238, Iteration: 1/2, Loss: 0.7463211666975822\n",
      "Epoch: 239, Iteration: 1/2, Loss: 0.9483142273069147\n",
      "Epoch: 240, Iteration: 1/2, Loss: 0.8487287187785333\n",
      "Epoch: 241, Iteration: 1/2, Loss: 0.8435857327088714\n",
      "Epoch: 242, Iteration: 1/2, Loss: 0.7414850678118412\n",
      "Epoch: 243, Iteration: 1/2, Loss: 0.8169030984323578\n",
      "Epoch: 244, Iteration: 1/2, Loss: 0.8254185472557656\n",
      "Epoch: 245, Iteration: 1/2, Loss: 0.8229519040530228\n",
      "Epoch: 246, Iteration: 1/2, Loss: 0.8157792446120882\n",
      "Epoch: 247, Iteration: 1/2, Loss: 0.812003206356023\n",
      "Epoch: 248, Iteration: 1/2, Loss: 0.8100742602087221\n",
      "Epoch: 249, Iteration: 1/2, Loss: 0.709452977241561\n",
      "Epoch: 250, Iteration: 1/2, Loss: 0.8989535816298513\n",
      "Epoch: 251, Iteration: 1/2, Loss: 0.7950026775160914\n",
      "Epoch: 252, Iteration: 1/2, Loss: 0.6992639496920033\n",
      "Epoch: 253, Iteration: 1/2, Loss: 0.8681962579677234\n",
      "Epoch: 254, Iteration: 1/2, Loss: 0.7861002614602624\n",
      "Epoch: 255, Iteration: 1/2, Loss: 0.8762010362416545\n",
      "Epoch: 256, Iteration: 1/2, Loss: 0.7708821892909106\n",
      "Epoch: 257, Iteration: 1/2, Loss: 0.7660541220221087\n",
      "Epoch: 258, Iteration: 1/2, Loss: 0.679607229359484\n",
      "Epoch: 259, Iteration: 1/2, Loss: 0.834312271820642\n",
      "Epoch: 260, Iteration: 1/2, Loss: 0.638916881026869\n",
      "Epoch: 261, Iteration: 1/2, Loss: 0.6802622193942304\n",
      "Epoch: 262, Iteration: 1/2, Loss: 0.8275109189337149\n",
      "Epoch: 263, Iteration: 1/2, Loss: 0.7657651156285152\n",
      "Epoch: 264, Iteration: 1/2, Loss: 0.6940590024953673\n",
      "Epoch: 265, Iteration: 1/2, Loss: 0.7540819841844654\n",
      "Epoch: 266, Iteration: 1/2, Loss: 0.7275787331083963\n",
      "Epoch: 267, Iteration: 1/2, Loss: 0.7856289861802519\n",
      "Epoch: 268, Iteration: 1/2, Loss: 0.6616499035420679\n",
      "Epoch: 269, Iteration: 1/2, Loss: 0.6909765081005996\n",
      "Epoch: 270, Iteration: 1/2, Loss: 0.7988973956322653\n",
      "Epoch: 271, Iteration: 1/2, Loss: 0.6144929129278083\n",
      "Epoch: 272, Iteration: 1/2, Loss: 0.753749231664862\n",
      "Epoch: 273, Iteration: 1/2, Loss: 0.6195323533662742\n",
      "Epoch: 274, Iteration: 1/2, Loss: 0.6855618041484495\n",
      "Epoch: 275, Iteration: 1/2, Loss: 0.7018185267326292\n",
      "Epoch: 276, Iteration: 1/2, Loss: 0.6971821615050047\n",
      "Epoch: 277, Iteration: 1/2, Loss: 0.6179844824233613\n",
      "Epoch: 278, Iteration: 1/2, Loss: 0.5692046729836688\n",
      "Epoch: 279, Iteration: 1/2, Loss: 0.7816700721625724\n",
      "Epoch: 280, Iteration: 1/2, Loss: 0.6953274016144442\n",
      "Epoch: 281, Iteration: 1/2, Loss: 0.6547737847848976\n",
      "Epoch: 282, Iteration: 1/2, Loss: 0.5366798544583831\n",
      "Epoch: 283, Iteration: 1/2, Loss: 0.8096367507027376\n",
      "Epoch: 284, Iteration: 1/2, Loss: 0.508298592594639\n",
      "Epoch: 285, Iteration: 1/2, Loss: 0.6270007244754765\n",
      "Epoch: 286, Iteration: 1/2, Loss: 0.5917711999666768\n",
      "Epoch: 287, Iteration: 1/2, Loss: 0.6363898246739363\n",
      "Epoch: 288, Iteration: 1/2, Loss: 0.6763881786384075\n",
      "Epoch: 289, Iteration: 1/2, Loss: 0.5670831470932578\n",
      "Epoch: 290, Iteration: 1/2, Loss: 0.5687499208089032\n",
      "Epoch: 291, Iteration: 1/2, Loss: 0.6280443347891601\n",
      "Epoch: 292, Iteration: 1/2, Loss: 0.6266533826400746\n",
      "Epoch: 293, Iteration: 1/2, Loss: 0.5094236601997996\n",
      "Epoch: 294, Iteration: 1/2, Loss: 0.6514402928222326\n",
      "Epoch: 295, Iteration: 1/2, Loss: 0.530608537637645\n",
      "Epoch: 296, Iteration: 1/2, Loss: 0.5895873989330216\n",
      "Epoch: 297, Iteration: 1/2, Loss: 0.5179122309690953\n",
      "Epoch: 298, Iteration: 1/2, Loss: 0.5612755412316076\n",
      "Epoch: 299, Iteration: 1/2, Loss: 0.49590859120091135\n",
      "Epoch: 300, Iteration: 1/2, Loss: 0.5828986961776425\n",
      "Epoch: 301, Iteration: 1/2, Loss: 0.47299704598244036\n",
      "Epoch: 302, Iteration: 1/2, Loss: 0.6308800776699021\n",
      "Epoch: 303, Iteration: 1/2, Loss: 0.5375503922442444\n",
      "Epoch: 304, Iteration: 1/2, Loss: 0.4391723350268827\n",
      "Epoch: 305, Iteration: 1/2, Loss: 0.7113211441294878\n",
      "Epoch: 306, Iteration: 1/2, Loss: 0.4505855613818556\n",
      "Epoch: 307, Iteration: 1/2, Loss: 0.4346688774325625\n",
      "Epoch: 308, Iteration: 1/2, Loss: 0.588691154377512\n",
      "Epoch: 309, Iteration: 1/2, Loss: 0.4769723177968833\n",
      "Epoch: 310, Iteration: 1/2, Loss: 0.6006970594787937\n",
      "Epoch: 311, Iteration: 1/2, Loss: 0.4270342271161772\n",
      "Epoch: 312, Iteration: 1/2, Loss: 0.5052501958344733\n",
      "Epoch: 313, Iteration: 1/2, Loss: 0.5852740370313251\n",
      "Epoch: 314, Iteration: 1/2, Loss: 0.2590170908631619\n",
      "Epoch: 315, Iteration: 1/2, Loss: 0.5189666162302213\n",
      "Epoch: 316, Iteration: 1/2, Loss: 0.6187825343815877\n",
      "Epoch: 317, Iteration: 1/2, Loss: 0.37780455163347254\n",
      "Epoch: 318, Iteration: 1/2, Loss: 0.503674018135049\n",
      "Epoch: 319, Iteration: 1/2, Loss: 0.3782725509316621\n",
      "Epoch: 320, Iteration: 1/2, Loss: 0.6139894564648184\n",
      "Epoch: 321, Iteration: 1/2, Loss: 0.383802293417953\n",
      "Epoch: 322, Iteration: 1/2, Loss: 0.3644745397425191\n",
      "Epoch: 323, Iteration: 1/2, Loss: 0.44107519028170666\n",
      "Epoch: 324, Iteration: 1/2, Loss: 0.5069259715272877\n",
      "Epoch: 325, Iteration: 1/2, Loss: 0.4493196837965414\n",
      "Epoch: 326, Iteration: 1/2, Loss: 0.4452658729643906\n",
      "Epoch: 327, Iteration: 1/2, Loss: 0.44003168909048584\n",
      "Epoch: 328, Iteration: 1/2, Loss: 0.4388208365414921\n",
      "Epoch: 329, Iteration: 1/2, Loss: 0.4673080241283662\n",
      "Epoch: 330, Iteration: 1/2, Loss: 0.4064092786332947\n",
      "Epoch: 331, Iteration: 1/2, Loss: 0.4565796829818648\n",
      "Epoch: 332, Iteration: 1/2, Loss: 0.5000827345095407\n",
      "Epoch: 333, Iteration: 1/2, Loss: 0.4111946048930357\n",
      "Epoch: 334, Iteration: 1/2, Loss: 0.3343442751596539\n",
      "Epoch: 335, Iteration: 1/2, Loss: 0.3918238696893068\n",
      "Epoch: 336, Iteration: 1/2, Loss: 0.4424589700753183\n",
      "Epoch: 337, Iteration: 1/2, Loss: 0.30973064835995134\n",
      "Epoch: 338, Iteration: 1/2, Loss: 0.5984822819832722\n",
      "Epoch: 339, Iteration: 1/2, Loss: 0.2716587519773044\n",
      "Epoch: 340, Iteration: 1/2, Loss: 0.40449551902331354\n",
      "Epoch: 341, Iteration: 1/2, Loss: 0.3235199736849822\n",
      "Epoch: 342, Iteration: 1/2, Loss: 0.567949665044668\n",
      "Epoch: 343, Iteration: 1/2, Loss: 0.30464538147668224\n",
      "Epoch: 344, Iteration: 1/2, Loss: 0.3925189611555512\n",
      "Epoch: 345, Iteration: 1/2, Loss: 0.3723905146336086\n",
      "Epoch: 346, Iteration: 1/2, Loss: 0.4110004955676049\n",
      "Epoch: 347, Iteration: 1/2, Loss: 0.3685205556788283\n",
      "Epoch: 348, Iteration: 1/2, Loss: 0.4892726863870116\n",
      "Epoch: 349, Iteration: 1/2, Loss: 0.288533912936645\n",
      "Epoch: 350, Iteration: 1/2, Loss: 0.38256253704890103\n",
      "Epoch: 351, Iteration: 1/2, Loss: 0.374648261976511\n",
      "Epoch: 352, Iteration: 1/2, Loss: 0.35768941471978255\n",
      "Epoch: 353, Iteration: 1/2, Loss: 0.47844747019169165\n",
      "Epoch: 354, Iteration: 1/2, Loss: 0.3606307531505456\n",
      "Epoch: 355, Iteration: 1/2, Loss: 0.2728201651244707\n",
      "Epoch: 356, Iteration: 1/2, Loss: 0.24602422133576496\n",
      "Epoch: 357, Iteration: 1/2, Loss: 0.4782040186842611\n",
      "Epoch: 358, Iteration: 1/2, Loss: 0.3668033468498254\n",
      "Epoch: 359, Iteration: 1/2, Loss: 0.4496125971040637\n",
      "Epoch: 360, Iteration: 1/2, Loss: 0.2596414370095081\n",
      "Epoch: 361, Iteration: 1/2, Loss: 0.4497537634393001\n",
      "Epoch: 362, Iteration: 1/2, Loss: 0.2567174262430648\n",
      "Epoch: 363, Iteration: 1/2, Loss: 0.35826086498256704\n",
      "Epoch: 364, Iteration: 1/2, Loss: 0.44340115805928837\n",
      "Epoch: 365, Iteration: 1/2, Loss: 0.23370152817342443\n",
      "Epoch: 366, Iteration: 1/2, Loss: 0.459256476699037\n",
      "Epoch: 367, Iteration: 1/2, Loss: 0.23250277361454597\n",
      "Epoch: 368, Iteration: 1/2, Loss: 0.2559766945855279\n",
      "Epoch: 369, Iteration: 1/2, Loss: 0.4415501818547045\n",
      "Epoch: 370, Iteration: 1/2, Loss: 0.3323665228366893\n",
      "Epoch: 371, Iteration: 1/2, Loss: 0.35412787350929154\n",
      "Epoch: 372, Iteration: 1/2, Loss: 0.32974520451163497\n",
      "Epoch: 373, Iteration: 1/2, Loss: 0.4497526631142858\n",
      "Epoch: 374, Iteration: 1/2, Loss: 0.23439300222206882\n",
      "Epoch: 375, Iteration: 1/2, Loss: 0.3242575660132323\n",
      "Epoch: 376, Iteration: 1/2, Loss: 0.33882423326737726\n",
      "Epoch: 377, Iteration: 1/2, Loss: 0.3369988598442772\n",
      "Epoch: 378, Iteration: 1/2, Loss: 0.45370199827326485\n",
      "Epoch: 379, Iteration: 1/2, Loss: 0.09598458730717233\n",
      "Epoch: 380, Iteration: 1/2, Loss: 0.3334303308861054\n",
      "Epoch: 381, Iteration: 1/2, Loss: 0.3072500475543978\n",
      "Epoch: 382, Iteration: 1/2, Loss: 0.5461388312093364\n",
      "Epoch: 383, Iteration: 1/2, Loss: 0.1989319118129574\n",
      "Epoch: 384, Iteration: 1/2, Loss: 0.4373355142446783\n",
      "Epoch: 385, Iteration: 1/2, Loss: 0.21778250898368998\n",
      "Epoch: 386, Iteration: 1/2, Loss: 0.21674513343399313\n",
      "Epoch: 387, Iteration: 1/2, Loss: 0.4101913005672665\n",
      "Epoch: 388, Iteration: 1/2, Loss: 0.22615655230469725\n",
      "Epoch: 389, Iteration: 1/2, Loss: 0.31141839883810685\n",
      "Epoch: 390, Iteration: 1/2, Loss: 0.41796194665478636\n",
      "Epoch: 391, Iteration: 1/2, Loss: 0.3101358710060794\n",
      "Epoch: 392, Iteration: 1/2, Loss: 0.3307859595366588\n",
      "Epoch: 393, Iteration: 1/2, Loss: 0.30866397985906857\n",
      "Epoch: 394, Iteration: 1/2, Loss: 0.31975984193973606\n",
      "Epoch: 395, Iteration: 1/2, Loss: 0.2172690704711598\n",
      "Epoch: 396, Iteration: 1/2, Loss: 0.4054173714812407\n",
      "Epoch: 397, Iteration: 1/2, Loss: 0.4333855406876327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 834/1000 [00:00<00:00, 1345.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 398, Iteration: 1/2, Loss: 0.18661691277119785\n",
      "Epoch: 399, Iteration: 1/2, Loss: 0.3147419229611564\n",
      "Epoch: 400, Iteration: 1/2, Loss: 0.3225905032399985\n",
      "Epoch: 401, Iteration: 1/2, Loss: 0.31275750949241793\n",
      "Epoch: 402, Iteration: 1/2, Loss: 0.2968390521230129\n",
      "Epoch: 403, Iteration: 1/2, Loss: 0.32469135675388616\n",
      "Epoch: 404, Iteration: 1/2, Loss: 0.2970431179435541\n",
      "Epoch: 405, Iteration: 1/2, Loss: 0.31870998221229285\n",
      "Epoch: 406, Iteration: 1/2, Loss: 0.30958461664526316\n",
      "Epoch: 407, Iteration: 1/2, Loss: 0.4104385771704787\n",
      "Epoch: 408, Iteration: 1/2, Loss: 0.19310974110816165\n",
      "Epoch: 409, Iteration: 1/2, Loss: 0.2972112906129574\n",
      "Epoch: 410, Iteration: 1/2, Loss: 0.4176542356688798\n",
      "Epoch: 411, Iteration: 1/2, Loss: 0.1824094030576047\n",
      "Epoch: 412, Iteration: 1/2, Loss: 0.3054187726929518\n",
      "Epoch: 413, Iteration: 1/2, Loss: 0.3047140906531655\n",
      "Epoch: 414, Iteration: 1/2, Loss: 0.3040405462284531\n",
      "Epoch: 415, Iteration: 1/2, Loss: 0.29813279716061913\n",
      "Epoch: 416, Iteration: 1/2, Loss: 0.30614973232760445\n",
      "Epoch: 417, Iteration: 1/2, Loss: 0.18941224920985655\n",
      "Epoch: 418, Iteration: 1/2, Loss: 0.2957165446861177\n",
      "Epoch: 419, Iteration: 1/2, Loss: 0.39766212810321455\n",
      "Epoch: 420, Iteration: 1/2, Loss: 0.2952397247995965\n",
      "Epoch: 421, Iteration: 1/2, Loss: 0.41012193661966434\n",
      "Epoch: 422, Iteration: 1/2, Loss: 0.18050582842479804\n",
      "Epoch: 423, Iteration: 1/2, Loss: 0.4045411094420479\n",
      "Epoch: 424, Iteration: 1/2, Loss: 0.28521463822482657\n",
      "Epoch: 425, Iteration: 1/2, Loss: 0.2846807284237659\n",
      "Epoch: 426, Iteration: 1/2, Loss: 0.17892039469820797\n",
      "Epoch: 427, Iteration: 1/2, Loss: 0.18526486222173244\n",
      "Epoch: 428, Iteration: 1/2, Loss: 0.27686178439167314\n",
      "Epoch: 429, Iteration: 1/2, Loss: 0.3998692239385781\n",
      "Epoch: 430, Iteration: 1/2, Loss: 0.2955069360682281\n",
      "Epoch: 431, Iteration: 1/2, Loss: 0.28669536175028654\n",
      "Epoch: 432, Iteration: 1/2, Loss: 0.300537491485294\n",
      "Epoch: 433, Iteration: 1/2, Loss: 0.28830218913008054\n",
      "Epoch: 434, Iteration: 1/2, Loss: 0.2991367152112804\n",
      "Epoch: 435, Iteration: 1/2, Loss: 0.39737639504929984\n",
      "Epoch: 436, Iteration: 1/2, Loss: 0.1695397139552649\n",
      "Epoch: 437, Iteration: 1/2, Loss: 0.2975306348434629\n",
      "Epoch: 438, Iteration: 1/2, Loss: 0.17708999459272784\n",
      "Epoch: 439, Iteration: 1/2, Loss: 0.3923485890773376\n",
      "Epoch: 440, Iteration: 1/2, Loss: 0.27961628426883084\n",
      "Epoch: 441, Iteration: 1/2, Loss: 0.294678216935246\n",
      "Epoch: 442, Iteration: 1/2, Loss: 0.2906604797252209\n",
      "Epoch: 443, Iteration: 1/2, Loss: 0.17955197695015962\n",
      "Epoch: 444, Iteration: 1/2, Loss: 0.27098138709873604\n",
      "Epoch: 445, Iteration: 1/2, Loss: 0.38896635754660364\n",
      "Epoch: 446, Iteration: 1/2, Loss: 0.40413191057058384\n",
      "Epoch: 447, Iteration: 1/2, Loss: 0.27025990020170876\n",
      "Epoch: 448, Iteration: 1/2, Loss: 0.16847101409820942\n",
      "Epoch: 449, Iteration: 1/2, Loss: 0.39301377651835057\n",
      "Epoch: 450, Iteration: 1/2, Loss: 0.05408200738942965\n",
      "Epoch: 451, Iteration: 1/2, Loss: 0.3824130485177968\n",
      "Epoch: 452, Iteration: 1/2, Loss: 0.396951960795781\n",
      "Epoch: 453, Iteration: 1/2, Loss: 0.2726892541424285\n",
      "Epoch: 454, Iteration: 1/2, Loss: 0.05222851970092405\n",
      "Epoch: 455, Iteration: 1/2, Loss: 0.49237337794161473\n",
      "Epoch: 456, Iteration: 1/2, Loss: 0.27630302778738836\n",
      "Epoch: 457, Iteration: 1/2, Loss: 0.1600963071391851\n",
      "Epoch: 458, Iteration: 1/2, Loss: 0.17038297842721728\n",
      "Epoch: 459, Iteration: 1/2, Loss: 0.266318987484058\n",
      "Epoch: 460, Iteration: 1/2, Loss: 0.3887745457687462\n",
      "Epoch: 461, Iteration: 1/2, Loss: 0.2752667995413789\n",
      "Epoch: 462, Iteration: 1/2, Loss: 0.17208341129491378\n",
      "Epoch: 463, Iteration: 1/2, Loss: 0.49923141119034947\n",
      "Epoch: 464, Iteration: 1/2, Loss: 0.15606066376666738\n",
      "Epoch: 465, Iteration: 1/2, Loss: 0.39088498297560104\n",
      "Epoch: 466, Iteration: 1/2, Loss: 0.15533070192448237\n",
      "Epoch: 467, Iteration: 1/2, Loss: 0.28630079904288813\n",
      "Epoch: 468, Iteration: 1/2, Loss: 0.16335983865705195\n",
      "Epoch: 469, Iteration: 1/2, Loss: 0.3794720042097661\n",
      "Epoch: 470, Iteration: 1/2, Loss: 0.394641573662959\n",
      "Epoch: 471, Iteration: 1/2, Loss: 0.04130751463946587\n",
      "Epoch: 472, Iteration: 1/2, Loss: 0.2709753738158228\n",
      "Epoch: 473, Iteration: 1/2, Loss: 0.3757989295794604\n",
      "Epoch: 474, Iteration: 1/2, Loss: 0.280133463501999\n",
      "Epoch: 475, Iteration: 1/2, Loss: 0.27964809032028765\n",
      "Epoch: 476, Iteration: 1/2, Loss: 0.16420788315929982\n",
      "Epoch: 477, Iteration: 1/2, Loss: 0.3805395428684566\n",
      "Epoch: 478, Iteration: 1/2, Loss: 0.27942894527214557\n",
      "Epoch: 479, Iteration: 1/2, Loss: 0.27544044857573063\n",
      "Epoch: 480, Iteration: 1/2, Loss: 0.16809799952896387\n",
      "Epoch: 481, Iteration: 1/2, Loss: 0.4871964477925793\n",
      "Epoch: 482, Iteration: 1/2, Loss: 0.1557592861960416\n",
      "Epoch: 483, Iteration: 1/2, Loss: 0.2779863748477146\n",
      "Epoch: 484, Iteration: 1/2, Loss: 0.27752116346574485\n",
      "Epoch: 485, Iteration: 1/2, Loss: 0.16123491072329657\n",
      "Epoch: 486, Iteration: 1/2, Loss: 0.37574073899950733\n",
      "Epoch: 487, Iteration: 1/2, Loss: 0.1639338485758143\n",
      "Epoch: 488, Iteration: 1/2, Loss: 0.4861417096873038\n",
      "Epoch: 489, Iteration: 1/2, Loss: 0.03974413051757876\n",
      "Epoch: 490, Iteration: 1/2, Loss: 0.26259314580420207\n",
      "Epoch: 491, Iteration: 1/2, Loss: 0.26236852373287545\n",
      "Epoch: 492, Iteration: 1/2, Loss: 0.3706687948360042\n",
      "Epoch: 493, Iteration: 1/2, Loss: 0.27634587559173296\n",
      "Epoch: 494, Iteration: 1/2, Loss: 0.1648278248738439\n",
      "Epoch: 495, Iteration: 1/2, Loss: 0.37419184328711586\n",
      "Epoch: 496, Iteration: 1/2, Loss: 0.2755156200410338\n",
      "Epoch: 497, Iteration: 1/2, Loss: 0.16016931692589892\n",
      "Epoch: 498, Iteration: 1/2, Loss: 0.25782749811666067\n",
      "Epoch: 499, Iteration: 1/2, Loss: 0.2640304265975124\n",
      "Epoch: 500, Iteration: 1/2, Loss: 0.3717098202253896\n",
      "Epoch: 501, Iteration: 1/2, Loss: 0.2708444554991025\n",
      "Epoch: 502, Iteration: 1/2, Loss: 0.2755552055401446\n",
      "Epoch: 503, Iteration: 1/2, Loss: 0.389119403501987\n",
      "Epoch: 504, Iteration: 1/2, Loss: 0.25684986828456946\n",
      "Epoch: 505, Iteration: 1/2, Loss: 0.1501984328880428\n",
      "Epoch: 506, Iteration: 1/2, Loss: 0.3851265383971716\n",
      "Epoch: 507, Iteration: 1/2, Loss: 0.14438356497808214\n",
      "Epoch: 508, Iteration: 1/2, Loss: 0.15981259795802316\n",
      "Epoch: 509, Iteration: 1/2, Loss: 0.3746365287288751\n",
      "Epoch: 510, Iteration: 1/2, Loss: 0.2724319708918482\n",
      "Epoch: 511, Iteration: 1/2, Loss: 0.2630900275087505\n",
      "Epoch: 512, Iteration: 1/2, Loss: 0.2759267318132842\n",
      "Epoch: 513, Iteration: 1/2, Loss: 0.3839583760068993\n",
      "Epoch: 514, Iteration: 1/2, Loss: 0.03073996282048827\n",
      "Epoch: 515, Iteration: 1/2, Loss: 0.37403031576554024\n",
      "Epoch: 516, Iteration: 1/2, Loss: 0.15764787886330534\n",
      "Epoch: 517, Iteration: 1/2, Loss: 0.3717131565600002\n",
      "Epoch: 518, Iteration: 1/2, Loss: 0.26886270222247\n",
      "Epoch: 519, Iteration: 1/2, Loss: 0.15775620432155466\n",
      "Epoch: 520, Iteration: 1/2, Loss: 0.3665530129826411\n",
      "Epoch: 521, Iteration: 1/2, Loss: 0.38836863715610864\n",
      "Epoch: 522, Iteration: 1/2, Loss: 0.14101192718414265\n",
      "Epoch: 523, Iteration: 1/2, Loss: 0.3857297401572418\n",
      "Epoch: 524, Iteration: 1/2, Loss: 0.14139125112807677\n",
      "Epoch: 525, Iteration: 1/2, Loss: 0.2728211871727662\n",
      "Epoch: 526, Iteration: 1/2, Loss: 0.2676738483970964\n",
      "Epoch: 527, Iteration: 1/2, Loss: 0.38310127512679026\n",
      "Epoch: 528, Iteration: 1/2, Loss: 0.25605280515505474\n",
      "Epoch: 529, Iteration: 1/2, Loss: 0.14162659584452195\n",
      "Epoch: 530, Iteration: 1/2, Loss: 0.3832656942329248\n",
      "Epoch: 531, Iteration: 1/2, Loss: 0.14174547817137378\n",
      "Epoch: 532, Iteration: 1/2, Loss: 0.26926083525585365\n",
      "Epoch: 533, Iteration: 1/2, Loss: 0.2715976693375185\n",
      "Epoch: 534, Iteration: 1/2, Loss: 0.26082276511500113\n",
      "Epoch: 535, Iteration: 1/2, Loss: 0.267344607911823\n",
      "Epoch: 536, Iteration: 1/2, Loss: 0.27170050766959775\n",
      "Epoch: 537, Iteration: 1/2, Loss: 0.3807283944861781\n",
      "Epoch: 538, Iteration: 1/2, Loss: 0.2546043936070386\n",
      "Epoch: 539, Iteration: 1/2, Loss: 0.14299588410048492\n",
      "Epoch: 540, Iteration: 1/2, Loss: 0.37935500927197807\n",
      "Epoch: 541, Iteration: 1/2, Loss: 0.14145891247303613\n",
      "Epoch: 542, Iteration: 1/2, Loss: 0.268639241856356\n",
      "Epoch: 543, Iteration: 1/2, Loss: 0.2657826782186547\n",
      "Epoch: 544, Iteration: 1/2, Loss: 0.38232187503181453\n",
      "Epoch: 545, Iteration: 1/2, Loss: 0.2514132443785497\n",
      "Epoch: 546, Iteration: 1/2, Loss: 0.14194298654345994\n",
      "Epoch: 547, Iteration: 1/2, Loss: 0.37872402998019994\n",
      "Epoch: 548, Iteration: 1/2, Loss: 0.2531213092014827\n",
      "Epoch: 549, Iteration: 1/2, Loss: 0.14224117559738866\n",
      "Epoch: 550, Iteration: 1/2, Loss: 0.2671558423535189\n",
      "Epoch: 551, Iteration: 1/2, Loss: 0.15076978675203362\n",
      "Epoch: 552, Iteration: 1/2, Loss: 0.25269712320158544\n",
      "Epoch: 553, Iteration: 1/2, Loss: 0.2529709269400862\n",
      "Epoch: 554, Iteration: 1/2, Loss: 0.2526707231650708\n",
      "Epoch: 555, Iteration: 1/2, Loss: 0.2503186385511596\n",
      "Epoch: 556, Iteration: 1/2, Loss: 0.36778703967226195\n",
      "Epoch: 557, Iteration: 1/2, Loss: 0.15042178184758195\n",
      "Epoch: 558, Iteration: 1/2, Loss: 0.36815573557743775\n",
      "Epoch: 559, Iteration: 1/2, Loss: 0.3782808176628525\n",
      "Epoch: 560, Iteration: 1/2, Loss: 0.13790464419266468\n",
      "Epoch: 561, Iteration: 1/2, Loss: 0.2663893594042581\n",
      "Epoch: 562, Iteration: 1/2, Loss: 0.26547652158213175\n",
      "Epoch: 563, Iteration: 1/2, Loss: 0.26484314283981086\n",
      "Epoch: 564, Iteration: 1/2, Loss: 0.3799855310022833\n",
      "Epoch: 565, Iteration: 1/2, Loss: 0.02285130660233043\n",
      "Epoch: 566, Iteration: 1/2, Loss: 0.3660351789416849\n",
      "Epoch: 567, Iteration: 1/2, Loss: 0.2656634889953003\n",
      "Epoch: 568, Iteration: 1/2, Loss: 0.26130805557528014\n",
      "Epoch: 569, Iteration: 1/2, Loss: 0.26735922422124025\n",
      "Epoch: 570, Iteration: 1/2, Loss: 0.263481551272463\n",
      "Epoch: 571, Iteration: 1/2, Loss: 0.3771503642327857\n",
      "Epoch: 572, Iteration: 1/2, Loss: 0.1364185392062739\n",
      "Epoch: 573, Iteration: 1/2, Loss: 0.2647171351547753\n",
      "Epoch: 574, Iteration: 1/2, Loss: 0.26501720399392725\n",
      "Epoch: 575, Iteration: 1/2, Loss: 0.2620558747502085\n",
      "Epoch: 576, Iteration: 1/2, Loss: 0.265235782823814\n",
      "Epoch: 577, Iteration: 1/2, Loss: 0.37730474016608195\n",
      "Epoch: 578, Iteration: 1/2, Loss: 0.2522697305873997\n",
      "Epoch: 579, Iteration: 1/2, Loss: 0.13568646504902912\n",
      "Epoch: 580, Iteration: 1/2, Loss: 0.26269928252073027\n",
      "Epoch: 581, Iteration: 1/2, Loss: 0.3769293419990636\n",
      "Epoch: 582, Iteration: 1/2, Loss: 0.24994477079474306\n",
      "Epoch: 583, Iteration: 1/2, Loss: 0.022997706718004336\n",
      "Epoch: 584, Iteration: 1/2, Loss: 0.36246612482736906\n",
      "Epoch: 585, Iteration: 1/2, Loss: 0.3775360832156148\n",
      "Epoch: 586, Iteration: 1/2, Loss: 0.13390111232631324\n",
      "Epoch: 587, Iteration: 1/2, Loss: 0.37848311309165983\n",
      "Epoch: 588, Iteration: 1/2, Loss: 0.2513020159894444\n",
      "Epoch: 589, Iteration: 1/2, Loss: 0.24751198911436798\n",
      "Epoch: 590, Iteration: 1/2, Loss: 0.13506519687507618\n",
      "Epoch: 591, Iteration: 1/2, Loss: 0.37688854267318106\n",
      "Epoch: 592, Iteration: 1/2, Loss: 0.021827156774344422\n",
      "Epoch: 593, Iteration: 1/2, Loss: 0.24897813700592705\n",
      "Epoch: 594, Iteration: 1/2, Loss: 0.36435182204883554\n",
      "Epoch: 595, Iteration: 1/2, Loss: 0.25995022526986267\n",
      "Epoch: 596, Iteration: 1/2, Loss: 0.2626247811839809\n",
      "Epoch: 597, Iteration: 1/2, Loss: 0.14731141177203463\n",
      "Epoch: 598, Iteration: 1/2, Loss: 0.36373303016711106\n",
      "Epoch: 599, Iteration: 1/2, Loss: 0.2614756510404319\n",
      "Epoch: 600, Iteration: 1/2, Loss: 0.26286607674782286\n",
      "Epoch: 601, Iteration: 1/2, Loss: 0.37568171371332704\n",
      "Epoch: 602, Iteration: 1/2, Loss: 0.13560767403310695\n",
      "Epoch: 603, Iteration: 1/2, Loss: 0.14726882701753252\n",
      "Epoch: 604, Iteration: 1/2, Loss: 0.2479561831695338\n",
      "Epoch: 605, Iteration: 1/2, Loss: 0.4756197198995085\n",
      "Epoch: 606, Iteration: 1/2, Loss: 0.020695082042744818\n",
      "Epoch: 607, Iteration: 1/2, Loss: 0.3611474193828765\n",
      "Epoch: 608, Iteration: 1/2, Loss: 0.2612937348131006\n",
      "Epoch: 609, Iteration: 1/2, Loss: 0.2630148901604309\n",
      "Epoch: 610, Iteration: 1/2, Loss: 0.3757947584453015\n",
      "Epoch: 611, Iteration: 1/2, Loss: 0.13226024446303156\n",
      "Epoch: 612, Iteration: 1/2, Loss: 0.3750041540558786\n",
      "Epoch: 613, Iteration: 1/2, Loss: 0.13488687336208988\n",
      "Epoch: 614, Iteration: 1/2, Loss: 0.26021664621015755\n",
      "Epoch: 615, Iteration: 1/2, Loss: 0.37462611385980027\n",
      "Epoch: 616, Iteration: 1/2, Loss: 0.13098519258828997\n",
      "Epoch: 617, Iteration: 1/2, Loss: 0.14951102090240828\n",
      "Epoch: 618, Iteration: 1/2, Loss: 0.47518270576325194\n",
      "Epoch: 619, Iteration: 1/2, Loss: 0.13432362995806274\n",
      "Epoch: 620, Iteration: 1/2, Loss: 0.2573416077645085\n",
      "Epoch: 621, Iteration: 1/2, Loss: 0.2620855647244802\n",
      "Epoch: 622, Iteration: 1/2, Loss: 0.2596448626869415\n",
      "Epoch: 623, Iteration: 1/2, Loss: 0.2630417621363802\n",
      "Epoch: 624, Iteration: 1/2, Loss: 0.14572194021631213\n",
      "Epoch: 625, Iteration: 1/2, Loss: 0.4744732744515322\n",
      "Epoch: 626, Iteration: 1/2, Loss: 0.24692610032246853\n",
      "Epoch: 627, Iteration: 1/2, Loss: 0.13095004763663076\n",
      "Epoch: 628, Iteration: 1/2, Loss: 0.14796458408639143\n",
      "Epoch: 629, Iteration: 1/2, Loss: 0.24629736789924017\n",
      "Epoch: 630, Iteration: 1/2, Loss: 0.47441456928503645\n",
      "Epoch: 631, Iteration: 1/2, Loss: 0.018613730429977975\n",
      "Epoch: 632, Iteration: 1/2, Loss: 0.35935260436508254\n",
      "Epoch: 633, Iteration: 1/2, Loss: 0.26230704553427175\n",
      "Epoch: 634, Iteration: 1/2, Loss: 0.2559962782063334\n",
      "Epoch: 635, Iteration: 1/2, Loss: 0.2624080835398661\n",
      "Epoch: 636, Iteration: 1/2, Loss: 0.2595715158080537\n",
      "Epoch: 637, Iteration: 1/2, Loss: 0.3736781993087989\n",
      "Epoch: 638, Iteration: 1/2, Loss: 0.018092917989638593\n",
      "Epoch: 639, Iteration: 1/2, Loss: 0.36186336767003924\n",
      "Epoch: 640, Iteration: 1/2, Loss: 0.37160812741129834\n",
      "Epoch: 641, Iteration: 1/2, Loss: 0.13416208408178193\n",
      "Epoch: 642, Iteration: 1/2, Loss: 0.2553568365399237\n",
      "Epoch: 643, Iteration: 1/2, Loss: 0.14577058224186165\n",
      "Epoch: 644, Iteration: 1/2, Loss: 0.36285591152250507\n",
      "Epoch: 645, Iteration: 1/2, Loss: 0.258776356547991\n",
      "Epoch: 646, Iteration: 1/2, Loss: 0.2614809557020284\n",
      "Epoch: 647, Iteration: 1/2, Loss: 0.25352454530628465\n",
      "Epoch: 648, Iteration: 1/2, Loss: 0.14750294802834904\n",
      "Epoch: 649, Iteration: 1/2, Loss: 0.3588408916204292\n",
      "Epoch: 650, Iteration: 1/2, Loss: 0.37369716966360816\n",
      "Epoch: 651, Iteration: 1/2, Loss: 0.017196780845034895\n",
      "Epoch: 652, Iteration: 1/2, Loss: 0.35975796123434856\n",
      "Epoch: 653, Iteration: 1/2, Loss: 0.25688644731997595\n",
      "Epoch: 654, Iteration: 1/2, Loss: 0.37443293804841815\n",
      "Epoch: 655, Iteration: 1/2, Loss: 0.13021499027058675\n",
      "Epoch: 656, Iteration: 1/2, Loss: 0.25960521649648016\n",
      "Epoch: 657, Iteration: 1/2, Loss: 0.3727680809325683\n",
      "Epoch: 658, Iteration: 1/2, Loss: 0.015370967149378693\n",
      "Epoch: 659, Iteration: 1/2, Loss: 0.35992250697949385\n",
      "Epoch: 660, Iteration: 1/2, Loss: 0.2595919544577299\n",
      "Epoch: 661, Iteration: 1/2, Loss: 0.3734412261778657\n",
      "Epoch: 662, Iteration: 1/2, Loss: 0.12920914897159322\n",
      "Epoch: 663, Iteration: 1/2, Loss: 0.2595965771989587\n",
      "Epoch: 664, Iteration: 1/2, Loss: 0.2554365843977212\n",
      "Epoch: 665, Iteration: 1/2, Loss: 0.3743339005068947\n",
      "Epoch: 666, Iteration: 1/2, Loss: 0.12989594468022445\n",
      "Epoch: 667, Iteration: 1/2, Loss: 0.26059597064514417\n",
      "Epoch: 668, Iteration: 1/2, Loss: 0.14087716557535518\n",
      "Epoch: 669, Iteration: 1/2, Loss: 0.357739148024862\n",
      "Epoch: 670, Iteration: 1/2, Loss: 0.25991511799504347\n",
      "Epoch: 671, Iteration: 1/2, Loss: 0.14297846281908214\n",
      "Epoch: 672, Iteration: 1/2, Loss: 0.35941365436685524\n",
      "Epoch: 673, Iteration: 1/2, Loss: 0.3725670195637841\n",
      "Epoch: 674, Iteration: 1/2, Loss: 0.12950417918657356\n",
      "Epoch: 675, Iteration: 1/2, Loss: 0.256537862758347\n",
      "Epoch: 676, Iteration: 1/2, Loss: 0.2601207520041246\n",
      "Epoch: 677, Iteration: 1/2, Loss: 0.25777693092094\n",
      "Epoch: 678, Iteration: 1/2, Loss: 0.14293162726233372\n",
      "Epoch: 679, Iteration: 1/2, Loss: 0.3594227166909867\n",
      "Epoch: 680, Iteration: 1/2, Loss: 0.141168347944686\n",
      "Epoch: 681, Iteration: 1/2, Loss: 0.2451933831789032\n",
      "Epoch: 682, Iteration: 1/2, Loss: 0.3603169292606892\n",
      "Epoch: 683, Iteration: 1/2, Loss: 0.3709088442439481\n",
      "Epoch: 684, Iteration: 1/2, Loss: 0.12567720844354796\n",
      "Epoch: 685, Iteration: 1/2, Loss: 0.3745383501195949\n",
      "Epoch: 686, Iteration: 1/2, Loss: 0.014925228943533332\n",
      "Epoch: 687, Iteration: 1/2, Loss: 0.35790424455970504\n",
      "Epoch: 688, Iteration: 1/2, Loss: 0.2572588524007562\n",
      "Epoch: 689, Iteration: 1/2, Loss: 0.2574275782484185\n",
      "Epoch: 690, Iteration: 1/2, Loss: 0.2562991880439346\n",
      "Epoch: 691, Iteration: 1/2, Loss: 0.25773080220565714\n",
      "Epoch: 692, Iteration: 1/2, Loss: 0.37220973054413165\n",
      "Epoch: 693, Iteration: 1/2, Loss: 0.2431934850544435\n",
      "Epoch: 694, Iteration: 1/2, Loss: 0.12746785714512432\n",
      "Epoch: 695, Iteration: 1/2, Loss: 0.2578669774233421\n",
      "Epoch: 696, Iteration: 1/2, Loss: 0.14352966691689373\n",
      "Epoch: 697, Iteration: 1/2, Loss: 0.3575897765715078\n",
      "Epoch: 698, Iteration: 1/2, Loss: 0.14262533121591\n",
      "Epoch: 699, Iteration: 1/2, Loss: 0.35658778523239054\n",
      "Epoch: 700, Iteration: 1/2, Loss: 0.14228302109712823\n",
      "Epoch: 701, Iteration: 1/2, Loss: 0.4731589617361606\n",
      "Epoch: 702, Iteration: 1/2, Loss: 0.12730872690890233\n",
      "Epoch: 703, Iteration: 1/2, Loss: 0.2578506601991423\n",
      "Epoch: 704, Iteration: 1/2, Loss: 0.25590181147965174\n",
      "Epoch: 705, Iteration: 1/2, Loss: 0.2578515716943273\n",
      "Epoch: 706, Iteration: 1/2, Loss: 0.14200253566820797\n",
      "Epoch: 707, Iteration: 1/2, Loss: 0.35658935312216916\n",
      "Epoch: 708, Iteration: 1/2, Loss: 0.3718415689739264\n",
      "Epoch: 709, Iteration: 1/2, Loss: 0.12881466215709725\n",
      "Epoch: 710, Iteration: 1/2, Loss: 0.37053279315118526\n",
      "Epoch: 711, Iteration: 1/2, Loss: 0.013728707526358118\n",
      "Epoch: 712, Iteration: 1/2, Loss: 0.3560690945128736\n",
      "Epoch: 713, Iteration: 1/2, Loss: 0.37204009629802337\n",
      "Epoch: 714, Iteration: 1/2, Loss: 0.12735070435660667\n",
      "Epoch: 715, Iteration: 1/2, Loss: 0.14169438693315048\n",
      "Epoch: 716, Iteration: 1/2, Loss: 0.35700250697542996\n",
      "Epoch: 717, Iteration: 1/2, Loss: 0.2574669575686519\n",
      "Epoch: 718, Iteration: 1/2, Loss: 0.3706663380933032\n",
      "Epoch: 719, Iteration: 1/2, Loss: 0.24262528188982416\n",
      "Epoch: 720, Iteration: 1/2, Loss: 0.12818730241181342\n",
      "Epoch: 721, Iteration: 1/2, Loss: 0.14135434368158414\n",
      "Epoch: 722, Iteration: 1/2, Loss: 0.35578663871180194\n",
      "Epoch: 723, Iteration: 1/2, Loss: 0.2572592070698237\n",
      "Epoch: 724, Iteration: 1/2, Loss: 0.25567267406894334\n",
      "Epoch: 725, Iteration: 1/2, Loss: 0.25641933352389285\n",
      "Epoch: 726, Iteration: 1/2, Loss: 0.3706362476425398\n",
      "Epoch: 727, Iteration: 1/2, Loss: 0.011958940583473548\n",
      "Epoch: 728, Iteration: 1/2, Loss: 0.3586353447688203\n",
      "Epoch: 729, Iteration: 1/2, Loss: 0.3705544825962901\n",
      "Epoch: 730, Iteration: 1/2, Loss: 0.12822556090129664\n",
      "Epoch: 731, Iteration: 1/2, Loss: 0.2516582506661562\n",
      "Epoch: 732, Iteration: 1/2, Loss: 0.25870237107895067\n",
      "Epoch: 733, Iteration: 1/2, Loss: 0.1410218582715457\n",
      "Epoch: 734, Iteration: 1/2, Loss: 0.354425764408257\n",
      "Epoch: 735, Iteration: 1/2, Loss: 0.257220808104968\n",
      "Epoch: 736, Iteration: 1/2, Loss: 0.25784519499560543\n",
      "Epoch: 737, Iteration: 1/2, Loss: 0.25189549342085493\n",
      "Epoch: 738, Iteration: 1/2, Loss: 0.2568165886432317\n",
      "Epoch: 739, Iteration: 1/2, Loss: 0.3712835228327245\n",
      "Epoch: 740, Iteration: 1/2, Loss: 0.12818502400264117\n",
      "Epoch: 741, Iteration: 1/2, Loss: 0.14009044556637013\n",
      "Epoch: 742, Iteration: 1/2, Loss: 0.4709617090554551\n",
      "Epoch: 743, Iteration: 1/2, Loss: 0.24166384455444107\n",
      "Epoch: 744, Iteration: 1/2, Loss: 0.12720915627252655\n",
      "Epoch: 745, Iteration: 1/2, Loss: 0.25743346298354597\n",
      "Epoch: 746, Iteration: 1/2, Loss: 0.36881511044283766\n",
      "Epoch: 747, Iteration: 1/2, Loss: 0.24075169979144573\n",
      "Epoch: 748, Iteration: 1/2, Loss: 0.24152684156080817\n",
      "Epoch: 749, Iteration: 1/2, Loss: 0.011184794647051226\n",
      "Epoch: 750, Iteration: 1/2, Loss: 0.47160534936780574\n",
      "Epoch: 751, Iteration: 1/2, Loss: 0.12837156958524615\n",
      "Epoch: 752, Iteration: 1/2, Loss: 0.25238839019658343\n",
      "Epoch: 753, Iteration: 1/2, Loss: 0.25770042085662864\n",
      "Epoch: 754, Iteration: 1/2, Loss: 0.3687823968639482\n",
      "Epoch: 755, Iteration: 1/2, Loss: 0.011172334119596755\n",
      "Epoch: 756, Iteration: 1/2, Loss: 0.3551616574842136\n",
      "Epoch: 757, Iteration: 1/2, Loss: 0.25754393808446435\n",
      "Epoch: 758, Iteration: 1/2, Loss: 0.25369149208813424\n",
      "Epoch: 759, Iteration: 1/2, Loss: 0.2561499159700522\n",
      "Epoch: 760, Iteration: 1/2, Loss: 0.25630859905008374\n",
      "Epoch: 761, Iteration: 1/2, Loss: 0.13791617568819267\n",
      "Epoch: 762, Iteration: 1/2, Loss: 0.24211677073302174\n",
      "Epoch: 763, Iteration: 1/2, Loss: 0.24110420021908463\n",
      "Epoch: 764, Iteration: 1/2, Loss: 0.35642004306624364\n",
      "Epoch: 765, Iteration: 1/2, Loss: 0.2566291794128013\n",
      "Epoch: 766, Iteration: 1/2, Loss: 0.25570650706166625\n",
      "Epoch: 767, Iteration: 1/2, Loss: 0.255757812738305\n",
      "Epoch: 768, Iteration: 1/2, Loss: 0.36565307148877857\n",
      "Epoch: 769, Iteration: 1/2, Loss: 0.12340020339484818\n",
      "Epoch: 770, Iteration: 1/2, Loss: 0.3733412901563156\n",
      "Epoch: 771, Iteration: 1/2, Loss: 0.009740775184325687\n",
      "Epoch: 772, Iteration: 1/2, Loss: 0.3573122650378989\n",
      "Epoch: 773, Iteration: 1/2, Loss: 0.25140052319662987\n",
      "Epoch: 774, Iteration: 1/2, Loss: 0.14161934713862057\n",
      "Epoch: 775, Iteration: 1/2, Loss: 0.3561369785417067\n",
      "Epoch: 776, Iteration: 1/2, Loss: 0.25488604531883097\n",
      "Epoch: 777, Iteration: 1/2, Loss: 0.2558007439186607\n",
      "Epoch: 778, Iteration: 1/2, Loss: 0.25074356236721274\n",
      "Epoch: 779, Iteration: 1/2, Loss: 0.3721467750257129\n",
      "Epoch: 780, Iteration: 1/2, Loss: 0.1254053652751233\n",
      "Epoch: 781, Iteration: 1/2, Loss: 0.25400092996692714\n",
      "Epoch: 782, Iteration: 1/2, Loss: 0.25435065714894084\n",
      "Epoch: 783, Iteration: 1/2, Loss: 0.2545902868756388\n",
      "Epoch: 784, Iteration: 1/2, Loss: 0.25649864431414393\n",
      "Epoch: 785, Iteration: 1/2, Loss: 0.25465787311401317\n",
      "Epoch: 786, Iteration: 1/2, Loss: 0.25107160922801813\n",
      "Epoch: 787, Iteration: 1/2, Loss: 0.14160427082952082\n",
      "Epoch: 788, Iteration: 1/2, Loss: 0.3539408489101812\n",
      "Epoch: 789, Iteration: 1/2, Loss: 0.2555318995257615\n",
      "Epoch: 790, Iteration: 1/2, Loss: 0.25549466436506046\n",
      "Epoch: 791, Iteration: 1/2, Loss: 0.2516383314775588\n",
      "Epoch: 792, Iteration: 1/2, Loss: 0.25727330349296396\n",
      "Epoch: 793, Iteration: 1/2, Loss: 0.3676486961830174\n",
      "Epoch: 794, Iteration: 1/2, Loss: 0.1276065791833255\n",
      "Epoch: 795, Iteration: 1/2, Loss: 0.25505879359896616\n",
      "Epoch: 796, Iteration: 1/2, Loss: 0.24787371025537375\n",
      "Epoch: 797, Iteration: 1/2, Loss: 0.25583710731263587\n",
      "Epoch: 798, Iteration: 1/2, Loss: 0.2556446104935281\n",
      "Epoch: 799, Iteration: 1/2, Loss: 0.13951450324247788\n",
      "Epoch: 800, Iteration: 1/2, Loss: 0.3564880295094634\n",
      "Epoch: 801, Iteration: 1/2, Loss: 0.36742740958053055\n",
      "Epoch: 802, Iteration: 1/2, Loss: 0.009803854352725194\n",
      "Epoch: 803, Iteration: 1/2, Loss: 0.3538830965994186\n",
      "Epoch: 804, Iteration: 1/2, Loss: 0.3706187396302497\n",
      "Epoch: 805, Iteration: 1/2, Loss: 0.12506879312082322\n",
      "Epoch: 806, Iteration: 1/2, Loss: 0.25342695997708703\n",
      "Epoch: 807, Iteration: 1/2, Loss: 0.2544561066482308\n",
      "Epoch: 808, Iteration: 1/2, Loss: 0.2547913092841194\n",
      "Epoch: 809, Iteration: 1/2, Loss: 0.13847908165944334\n",
      "Epoch: 810, Iteration: 1/2, Loss: 0.4695502068682554\n",
      "Epoch: 811, Iteration: 1/2, Loss: 0.12697616371722686\n",
      "Epoch: 812, Iteration: 1/2, Loss: 0.13727410065469628\n",
      "Epoch: 813, Iteration: 1/2, Loss: 0.4695081133646211\n",
      "Epoch: 814, Iteration: 1/2, Loss: 0.12723091812950535\n",
      "Epoch: 815, Iteration: 1/2, Loss: 0.36664031679442544\n",
      "Epoch: 816, Iteration: 1/2, Loss: 0.1273002873314101\n",
      "Epoch: 817, Iteration: 1/2, Loss: 0.13657747506990395\n",
      "Epoch: 818, Iteration: 1/2, Loss: 0.4702739947133151\n",
      "Epoch: 819, Iteration: 1/2, Loss: 0.12583391349477666\n",
      "Epoch: 820, Iteration: 1/2, Loss: 0.25513580263839175\n",
      "Epoch: 821, Iteration: 1/2, Loss: 0.1356591719623327\n",
      "Epoch: 822, Iteration: 1/2, Loss: 0.23993278099801868\n",
      "Epoch: 823, Iteration: 1/2, Loss: 0.46936806574447276\n",
      "Epoch: 824, Iteration: 1/2, Loss: 0.127388010508759\n",
      "Epoch: 825, Iteration: 1/2, Loss: 0.1361524709666477\n",
      "Epoch: 826, Iteration: 1/2, Loss: 0.46945936109837555\n",
      "Epoch: 827, Iteration: 1/2, Loss: 0.12710866012113217\n",
      "Epoch: 828, Iteration: 1/2, Loss: 0.24887542126297252\n",
      "Epoch: 829, Iteration: 1/2, Loss: 0.1410877929773194\n",
      "Epoch: 830, Iteration: 1/2, Loss: 0.35574224514096364\n",
      "Epoch: 831, Iteration: 1/2, Loss: 0.2496839664206797\n",
      "Epoch: 832, Iteration: 1/2, Loss: 0.25496106812195096\n",
      "Epoch: 833, Iteration: 1/2, Loss: 0.3692891299743929\n",
      "Epoch: 834, Iteration: 1/2, Loss: 0.1243428214225605\n",
      "Epoch: 835, Iteration: 1/2, Loss: 0.2525556722758978\n",
      "Epoch: 836, Iteration: 1/2, Loss: 0.25429592387793687\n",
      "Epoch: 837, Iteration: 1/2, Loss: 0.25305812617804324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1000/1000 [00:00<00:00, 1426.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 838, Iteration: 1/2, Loss: 0.25278671160339794\n",
      "Epoch: 839, Iteration: 1/2, Loss: 0.36975564099329183\n",
      "Epoch: 840, Iteration: 1/2, Loss: 0.2387453523348126\n",
      "Epoch: 841, Iteration: 1/2, Loss: 0.12438255343255133\n",
      "Epoch: 842, Iteration: 1/2, Loss: 0.13837802716141398\n",
      "Epoch: 843, Iteration: 1/2, Loss: 0.35388564370804276\n",
      "Epoch: 844, Iteration: 1/2, Loss: 0.25436192313811\n",
      "Epoch: 845, Iteration: 1/2, Loss: 0.13730899079845427\n",
      "Epoch: 846, Iteration: 1/2, Loss: 0.4692708373404683\n",
      "Epoch: 847, Iteration: 1/2, Loss: 0.12502260826289344\n",
      "Epoch: 848, Iteration: 1/2, Loss: 0.25467747813250047\n",
      "Epoch: 849, Iteration: 1/2, Loss: 0.2533076089047798\n",
      "Epoch: 850, Iteration: 1/2, Loss: 0.24700734304670663\n",
      "Epoch: 851, Iteration: 1/2, Loss: 0.25892697730478426\n",
      "Epoch: 852, Iteration: 1/2, Loss: 0.36661981566124724\n",
      "Epoch: 853, Iteration: 1/2, Loss: 0.008499804537319721\n",
      "Epoch: 854, Iteration: 1/2, Loss: 0.23913473971473923\n",
      "Epoch: 855, Iteration: 1/2, Loss: 0.46900815579811167\n",
      "Epoch: 856, Iteration: 1/2, Loss: 0.23983585710817365\n",
      "Epoch: 857, Iteration: 1/2, Loss: 0.008566653698161283\n",
      "Epoch: 858, Iteration: 1/2, Loss: 0.35344682737903876\n",
      "Epoch: 859, Iteration: 1/2, Loss: 0.3683075138084021\n",
      "Epoch: 860, Iteration: 1/2, Loss: 0.00850254327982882\n",
      "Epoch: 861, Iteration: 1/2, Loss: 0.4694891386281099\n",
      "Epoch: 862, Iteration: 1/2, Loss: 0.1247008293414705\n",
      "Epoch: 863, Iteration: 1/2, Loss: 0.2506315340990324\n",
      "Epoch: 864, Iteration: 1/2, Loss: 0.1396541089954444\n",
      "Epoch: 865, Iteration: 1/2, Loss: 0.2389121471220019\n",
      "Epoch: 866, Iteration: 1/2, Loss: 0.35337172471364564\n",
      "Epoch: 867, Iteration: 1/2, Loss: 0.2547876927296865\n",
      "Epoch: 868, Iteration: 1/2, Loss: 0.2495936327048969\n",
      "Epoch: 869, Iteration: 1/2, Loss: 0.3695438940782595\n",
      "Epoch: 870, Iteration: 1/2, Loss: 0.1245985472787204\n",
      "Epoch: 871, Iteration: 1/2, Loss: 0.13632844917970272\n",
      "Epoch: 872, Iteration: 1/2, Loss: 0.23953919499960616\n",
      "Epoch: 873, Iteration: 1/2, Loss: 0.35505204296157533\n",
      "Epoch: 874, Iteration: 1/2, Loss: 0.13646419146353633\n",
      "Epoch: 875, Iteration: 1/2, Loss: 0.3524487229795381\n",
      "Epoch: 876, Iteration: 1/2, Loss: 0.2539698434595318\n",
      "Epoch: 877, Iteration: 1/2, Loss: 0.13699694916564548\n",
      "Epoch: 878, Iteration: 1/2, Loss: 0.23928612453637785\n",
      "Epoch: 879, Iteration: 1/2, Loss: 0.3549812529973858\n",
      "Epoch: 880, Iteration: 1/2, Loss: 0.2537144160079892\n",
      "Epoch: 881, Iteration: 1/2, Loss: 0.13540499412002685\n",
      "Epoch: 882, Iteration: 1/2, Loss: 0.3510862498938273\n",
      "Epoch: 883, Iteration: 1/2, Loss: 0.14017601596695875\n",
      "Epoch: 884, Iteration: 1/2, Loss: 0.3531179654135272\n",
      "Epoch: 885, Iteration: 1/2, Loss: 0.36788477840322165\n",
      "Epoch: 886, Iteration: 1/2, Loss: 0.008716039424401651\n",
      "Epoch: 887, Iteration: 1/2, Loss: 0.35354862654777697\n",
      "Epoch: 888, Iteration: 1/2, Loss: 0.13751011421570802\n",
      "Epoch: 889, Iteration: 1/2, Loss: 0.23783740274116982\n",
      "Epoch: 890, Iteration: 1/2, Loss: 0.3553888757278303\n",
      "Epoch: 891, Iteration: 1/2, Loss: 0.2527211825133297\n",
      "Epoch: 892, Iteration: 1/2, Loss: 0.2532825022463085\n",
      "Epoch: 893, Iteration: 1/2, Loss: 0.36521510065377544\n",
      "Epoch: 894, Iteration: 1/2, Loss: 0.12606240413792094\n",
      "Epoch: 895, Iteration: 1/2, Loss: 0.253663068510615\n",
      "Epoch: 896, Iteration: 1/2, Loss: 0.3637554734611223\n",
      "Epoch: 897, Iteration: 1/2, Loss: 0.11986119273512771\n",
      "Epoch: 898, Iteration: 1/2, Loss: 0.14035827668965004\n",
      "Epoch: 899, Iteration: 1/2, Loss: 0.23893867744896463\n",
      "Epoch: 900, Iteration: 1/2, Loss: 0.35194342351222385\n",
      "Epoch: 901, Iteration: 1/2, Loss: 0.2535801885015482\n",
      "Epoch: 902, Iteration: 1/2, Loss: 0.13745062792877985\n",
      "Epoch: 903, Iteration: 1/2, Loss: 0.3540642873279245\n",
      "Epoch: 904, Iteration: 1/2, Loss: 0.13666546947023392\n",
      "Epoch: 905, Iteration: 1/2, Loss: 0.35456242897518997\n",
      "Epoch: 906, Iteration: 1/2, Loss: 0.2529772937878089\n",
      "Epoch: 907, Iteration: 1/2, Loss: 0.1353037490617321\n",
      "Epoch: 908, Iteration: 1/2, Loss: 0.35549083478379956\n",
      "Epoch: 909, Iteration: 1/2, Loss: 0.3652033075370298\n",
      "Epoch: 910, Iteration: 1/2, Loss: 0.1262799612130355\n",
      "Epoch: 911, Iteration: 1/2, Loss: 0.3641022788065226\n",
      "Epoch: 912, Iteration: 1/2, Loss: 0.0075781543097642266\n",
      "Epoch: 913, Iteration: 1/2, Loss: 0.23822415343910597\n",
      "Epoch: 914, Iteration: 1/2, Loss: 0.2387155992692925\n",
      "Epoch: 915, Iteration: 1/2, Loss: 0.35457665666620286\n",
      "Epoch: 916, Iteration: 1/2, Loss: 0.13586025508953764\n",
      "Epoch: 917, Iteration: 1/2, Loss: 0.23814653188403534\n",
      "Epoch: 918, Iteration: 1/2, Loss: 0.3552749012439265\n",
      "Epoch: 919, Iteration: 1/2, Loss: 0.13495549548589775\n",
      "Epoch: 920, Iteration: 1/2, Loss: 0.35119626537976445\n",
      "Epoch: 921, Iteration: 1/2, Loss: 0.3689468031420854\n",
      "Epoch: 922, Iteration: 1/2, Loss: 0.12394081660313759\n",
      "Epoch: 923, Iteration: 1/2, Loss: 0.2525109632929906\n",
      "Epoch: 924, Iteration: 1/2, Loss: 0.3658277575085209\n",
      "Epoch: 925, Iteration: 1/2, Loss: 0.007880834462572213\n",
      "Epoch: 926, Iteration: 1/2, Loss: 0.3549868316610935\n",
      "Epoch: 927, Iteration: 1/2, Loss: 0.13502264142487802\n",
      "Epoch: 928, Iteration: 1/2, Loss: 0.3548842669238056\n",
      "Epoch: 929, Iteration: 1/2, Loss: 0.1350969433218428\n",
      "Epoch: 930, Iteration: 1/2, Loss: 0.4681461126137659\n",
      "Epoch: 931, Iteration: 1/2, Loss: 0.12507712131486995\n",
      "Epoch: 932, Iteration: 1/2, Loss: 0.25272903960605253\n",
      "Epoch: 933, Iteration: 1/2, Loss: 0.2457202944244484\n",
      "Epoch: 934, Iteration: 1/2, Loss: 0.14008600810779476\n",
      "Epoch: 935, Iteration: 1/2, Loss: 0.3546926829387901\n",
      "Epoch: 936, Iteration: 1/2, Loss: 0.2522370449960968\n",
      "Epoch: 937, Iteration: 1/2, Loss: 0.24641808533155213\n",
      "Epoch: 938, Iteration: 1/2, Loss: 0.14016057464258388\n",
      "Epoch: 939, Iteration: 1/2, Loss: 0.3518052847504731\n",
      "Epoch: 940, Iteration: 1/2, Loss: 0.13769887637873943\n",
      "Epoch: 941, Iteration: 1/2, Loss: 0.35317045263361\n",
      "Epoch: 942, Iteration: 1/2, Loss: 0.2523726998879587\n",
      "Epoch: 943, Iteration: 1/2, Loss: 0.3659930331737624\n",
      "Epoch: 944, Iteration: 1/2, Loss: 0.007581748160311857\n",
      "Epoch: 945, Iteration: 1/2, Loss: 0.3545547406810299\n",
      "Epoch: 946, Iteration: 1/2, Loss: 0.3651586777335703\n",
      "Epoch: 947, Iteration: 1/2, Loss: 0.12484562934150634\n",
      "Epoch: 948, Iteration: 1/2, Loss: 0.24715967659077465\n",
      "Epoch: 949, Iteration: 1/2, Loss: 0.3693056289559495\n",
      "Epoch: 950, Iteration: 1/2, Loss: 0.12176205547698171\n",
      "Epoch: 951, Iteration: 1/2, Loss: 0.25243154366372406\n",
      "Epoch: 952, Iteration: 1/2, Loss: 0.13660733441418632\n",
      "Epoch: 953, Iteration: 1/2, Loss: 0.35366964832397646\n",
      "Epoch: 954, Iteration: 1/2, Loss: 0.2493208069289449\n",
      "Epoch: 955, Iteration: 1/2, Loss: 0.2528297617659778\n",
      "Epoch: 956, Iteration: 1/2, Loss: 0.25119324918430774\n",
      "Epoch: 957, Iteration: 1/2, Loss: 0.13694244027199398\n",
      "Epoch: 958, Iteration: 1/2, Loss: 0.3526375606159861\n",
      "Epoch: 959, Iteration: 1/2, Loss: 0.1360340641363151\n",
      "Epoch: 960, Iteration: 1/2, Loss: 0.35280729171105446\n",
      "Epoch: 961, Iteration: 1/2, Loss: 0.25165898057188885\n",
      "Epoch: 962, Iteration: 1/2, Loss: 0.2509480626574787\n",
      "Epoch: 963, Iteration: 1/2, Loss: 0.2515952907318024\n",
      "Epoch: 964, Iteration: 1/2, Loss: 0.3679207829074521\n",
      "Epoch: 965, Iteration: 1/2, Loss: 0.1226187763348653\n",
      "Epoch: 966, Iteration: 1/2, Loss: 0.24957038207914567\n",
      "Epoch: 967, Iteration: 1/2, Loss: 0.367802812715172\n",
      "Epoch: 968, Iteration: 1/2, Loss: 0.23747420189476107\n",
      "Epoch: 969, Iteration: 1/2, Loss: 0.12219018456378641\n",
      "Epoch: 970, Iteration: 1/2, Loss: 0.25248550897571054\n",
      "Epoch: 971, Iteration: 1/2, Loss: 0.1353873267617751\n",
      "Epoch: 972, Iteration: 1/2, Loss: 0.3539389247040353\n",
      "Epoch: 973, Iteration: 1/2, Loss: 0.36512069193003865\n",
      "Epoch: 974, Iteration: 1/2, Loss: 0.12427291708996013\n",
      "Epoch: 975, Iteration: 1/2, Loss: 0.13382012798648554\n",
      "Epoch: 976, Iteration: 1/2, Loss: 0.2380186243367836\n",
      "Epoch: 977, Iteration: 1/2, Loss: 0.3543084339163077\n",
      "Epoch: 978, Iteration: 1/2, Loss: 0.364671220369327\n",
      "Epoch: 979, Iteration: 1/2, Loss: 0.2379586139036757\n",
      "Epoch: 980, Iteration: 1/2, Loss: 0.11995258777511812\n",
      "Epoch: 981, Iteration: 1/2, Loss: 0.25269215917480425\n",
      "Epoch: 982, Iteration: 1/2, Loss: 0.13646261866876952\n",
      "Epoch: 983, Iteration: 1/2, Loss: 0.23685470254791471\n",
      "Epoch: 984, Iteration: 1/2, Loss: 0.3527706883825776\n",
      "Epoch: 985, Iteration: 1/2, Loss: 0.2525284937421694\n",
      "Epoch: 986, Iteration: 1/2, Loss: 0.3658544697619569\n",
      "Epoch: 987, Iteration: 1/2, Loss: 0.12282508667659342\n",
      "Epoch: 988, Iteration: 1/2, Loss: 0.13439005705466645\n",
      "Epoch: 989, Iteration: 1/2, Loss: 0.3545174722727343\n",
      "Epoch: 990, Iteration: 1/2, Loss: 0.134570527569024\n",
      "Epoch: 991, Iteration: 1/2, Loss: 0.354858771501113\n",
      "Epoch: 992, Iteration: 1/2, Loss: 0.25141472761712247\n",
      "Epoch: 993, Iteration: 1/2, Loss: 0.24477106979128876\n",
      "Epoch: 994, Iteration: 1/2, Loss: 0.25678071558504\n",
      "Epoch: 995, Iteration: 1/2, Loss: 0.3645581665614915\n",
      "Epoch: 996, Iteration: 1/2, Loss: 0.11999405366783158\n",
      "Epoch: 997, Iteration: 1/2, Loss: 0.2525652955238399\n",
      "Epoch: 998, Iteration: 1/2, Loss: 0.3674321197822339\n",
      "Epoch: 999, Iteration: 1/2, Loss: 0.12154876310883136\n",
      "Epoch: 1000, Iteration: 1/2, Loss: 0.1358151451556703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# configurations\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "# preprocess\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word2idx, idx2word = preprocess([text])\n",
    "contexts, target = create_contexts_target(corpus[0], window_size)\n",
    "target = convert_one_hot(target, len(word2idx))\n",
    "contexts = convert_one_hot(contexts, len(word2idx))\n",
    "\n",
    "# define model\n",
    "cbow_model = CBOW(vocab_size=len(word2idx), hidden_size=hidden_size, window_size=window_size)\n",
    "sgd_optimizer = SGD()\n",
    "trainer = Trainer(cbow_model, sgd_optimizer)\n",
    "\n",
    "# start training\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and [-0.92114466  1.1788381   0.26798314 -2.1704865  -0.974366  ]\n",
      "goodbye [ 1.8949281  -0.81951654 -0.5236294  -0.7115823  -0.16170278]\n",
      "you [-0.21078716 -2.6440973   1.1406666   1.2014605  -0.14241564]\n",
      "hello [-0.21707328 -2.6437354   1.1188228   1.2002566  -0.13221307]\n",
      "say [-1.166946    1.8818454   0.42215368 -0.07854651  0.69365376]\n",
      "i [ 1.8955749  -0.82273084 -0.5301938  -0.69270855 -0.16498493]\n",
      ". [-0.2630915   0.7213147   0.17421162  2.1127164   1.6681751 ]\n"
     ]
    }
   ],
   "source": [
    "# check word2vec results\n",
    "word_vecs = cbow_model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Derivative Equation\n",
    "\n",
    "1. **Softmax with Cross Entropy:**\n",
    "$$\n",
    "\\begin{align}\n",
    "Softmax &= \\frac{e^{z_i}}{\\sum_j^ke^{z_j}} \\\\\n",
    "Cross\\ Entropy &= -\\frac{1}{N}\\sum_j^N\\sum_i^c(\\hat{y_i}logy_i)\n",
    "\\end{align}\n",
    "$$\n",
    "在進行分類問題時，很常使用的output層函數是softmax，而此函數也經常與cross entropy的損失函數一起使用(學員可以參考此[文章](https://medium.com/jarvis-toward-intelligence/%E6%AF%94%E8%BC%83-cross-entropy-%E8%88%87-mean-squared-error-8bebc0255f5)了解為何分類為題常使用cross entropy當損失函數)。 在word2vec模型的訓練，也可以看成是一種分類問題(分類字詞)，因此訓練時的輸出層也是使用softmax與cross entropy。 相關的導函數推導可以參考此兩篇文章 [reference-1](https://medium.com/hoskiss-stand/backpropagation-with-softmax-cross-entropy-d60983b7b245), [reference-2](https://zhuanlan.zhihu.com/p/25723112)\n",
    "\n",
    "\n",
    "2. **Dense layer:**\n",
    "\n",
    "$$\n",
    "y = xW\n",
    "$$\n",
    "\n",
    "在導函數的推導上，以舉例進行解說。\n",
    "\n",
    "Ex:\n",
    "$$\n",
    "x= \\left[\n",
    "\\begin{matrix}\n",
    "    x_1&x_2&x_3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "$$\n",
    "W= \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}&W_{12}&W_{13} \\\\\n",
    "    W_{21}&W_{22}&W_{23} \\\\\n",
    "    W_{31}&W_{32}&W_{331}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = xW = \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}x_1+W_{21}x_2+W_{31}x_3&W_{12}x_1+W_{22}x_2+W_{32}x_3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial{y_1}}{\\partial{x_1}}&\\frac{\\partial{y_1}}{\\partial{x_2}}&\\frac{\\partial{y_1}}{\\partial{x_3}} \\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}}&\\frac{\\partial{y_2}}{\\partial{x_2}}&\\frac{\\partial{y_2}}{\\partial{x_3}}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}&W_{21}&W_{31} \\\\\n",
    "    W_{12}&W_{22}&W_{32}\n",
    "\\end{matrix}\n",
    "\\right] = W^T\n",
    "$$\n",
    "\n",
    "因此損失函數L對x的微分為\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}W^T\n",
    "$$\n",
    "\n",
    "同理損失函數L對y的微分為\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{W}} = x^T\\frac{\\partial{L}}{\\partial{y}}\n",
    "$$\n",
    "\n",
    "對於矩陣微分想要更瞭解的學員可以參考此[文章](https://zhuanlan.zhihu.com/p/34826167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
