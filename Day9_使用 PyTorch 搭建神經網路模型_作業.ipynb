{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0745, 0.1650, 0.0368, 0.1430, 0.0909, 0.0640, 0.0819, 0.0563, 0.1683,\n",
      "         0.1192],\n",
      "        [0.0590, 0.0593, 0.0590, 0.0563, 0.0916, 0.0671, 0.0470, 0.1618, 0.1404,\n",
      "         0.2585],\n",
      "        [0.0953, 0.0631, 0.0911, 0.0448, 0.1053, 0.1130, 0.1365, 0.0933, 0.1094,\n",
      "         0.1483],\n",
      "        [0.0838, 0.0953, 0.0655, 0.0877, 0.0938, 0.0915, 0.1105, 0.1290, 0.0902,\n",
      "         0.1526]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3618, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = NLLLoss()\n",
    "loss = criterion(torch.log(output), target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3618, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(torch.log(output), target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0478,  0.0341,  0.0029,  ...,  0.0571, -0.0068, -0.0381],\n",
      "        [ 0.0192,  0.0503,  0.0244,  ..., -0.0235,  0.0092,  0.0015],\n",
      "        [-0.0625,  0.0586,  0.0055,  ..., -0.0138,  0.0270,  0.0242],\n",
      "        ...,\n",
      "        [ 0.0623, -0.0507, -0.0317,  ..., -0.0457, -0.0431, -0.0385],\n",
      "        [-0.0440, -0.0611,  0.0355,  ..., -0.0441, -0.0460,  0.0226],\n",
      "        [-0.0303,  0.0565,  0.0513,  ...,  0.0221, -0.0123, -0.0329]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-1.4388e-02,  4.0130e-03,  6.7798e-02,  ...,  5.8259e-03,\n",
      "          1.0939e-02, -2.0396e-03],\n",
      "        [-6.6714e-02, -2.9093e-01, -8.4435e-01,  ...,  2.3720e-01,\n",
      "         -6.2348e-01, -3.8151e-01],\n",
      "        [-1.4267e-03,  1.8643e-03,  9.2419e-03,  ...,  1.0470e-02,\n",
      "         -1.8479e-02,  2.8169e-04],\n",
      "        ...,\n",
      "        [ 9.7395e-02,  7.4536e-02, -7.8448e-02,  ..., -3.3999e-02,\n",
      "          6.8368e-03,  1.1823e-01],\n",
      "        [-3.9688e-06, -2.5760e-06, -2.4402e-07,  ...,  1.8225e-06,\n",
      "         -4.5626e-06, -2.1131e-06],\n",
      "        [ 3.0636e-02,  1.2599e-02, -3.4524e-02,  ..., -4.2560e-02,\n",
      "          7.5556e-02,  1.9716e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0468,  0.0331,  0.0019,  ...,  0.0561, -0.0078, -0.0371],\n",
      "        [ 0.0202,  0.0513,  0.0254,  ..., -0.0245,  0.0102,  0.0025],\n",
      "        [-0.0615,  0.0576,  0.0045,  ..., -0.0148,  0.0280,  0.0232],\n",
      "        ...,\n",
      "        [ 0.0613, -0.0517, -0.0307,  ..., -0.0447, -0.0441, -0.0395],\n",
      "        [-0.0430, -0.0601,  0.0365,  ..., -0.0451, -0.0450,  0.0236],\n",
      "        [-0.0313,  0.0555,  0.0523,  ...,  0.0231, -0.0133, -0.0339]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-1.4388e-02,  4.0130e-03,  6.7798e-02,  ...,  5.8259e-03,\n",
      "          1.0939e-02, -2.0396e-03],\n",
      "        [-6.6714e-02, -2.9093e-01, -8.4435e-01,  ...,  2.3720e-01,\n",
      "         -6.2348e-01, -3.8151e-01],\n",
      "        [-1.4267e-03,  1.8643e-03,  9.2419e-03,  ...,  1.0470e-02,\n",
      "         -1.8479e-02,  2.8169e-04],\n",
      "        ...,\n",
      "        [ 9.7395e-02,  7.4536e-02, -7.8448e-02,  ..., -3.3999e-02,\n",
      "          6.8368e-03,  1.1823e-01],\n",
      "        [-3.9688e-06, -2.5760e-06, -2.4402e-07,  ...,  1.8225e-06,\n",
      "         -4.5626e-06, -2.1131e-06],\n",
      "        [ 3.0636e-02,  1.2599e-02, -3.4524e-02,  ..., -4.2560e-02,\n",
      "          7.5556e-02,  1.9716e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0468,  0.0331,  0.0019,  ...,  0.0561, -0.0078, -0.0371],\n",
      "        [ 0.0202,  0.0513,  0.0254,  ..., -0.0245,  0.0102,  0.0025],\n",
      "        [-0.0615,  0.0576,  0.0045,  ..., -0.0148,  0.0280,  0.0232],\n",
      "        ...,\n",
      "        [ 0.0613, -0.0517, -0.0307,  ..., -0.0447, -0.0441, -0.0395],\n",
      "        [-0.0430, -0.0601,  0.0365,  ..., -0.0451, -0.0450,  0.0236],\n",
      "        [-0.0313,  0.0555,  0.0523,  ...,  0.0231, -0.0133, -0.0339]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
