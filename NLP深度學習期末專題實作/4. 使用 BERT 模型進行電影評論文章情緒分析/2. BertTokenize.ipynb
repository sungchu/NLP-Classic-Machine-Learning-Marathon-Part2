{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"2-BertTokenize.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_Z3MjUoYgcr4"},"source":["載入模型所必須要的相依套件"]},{"cell_type":"code","metadata":{"id":"tzQKNAg7d_jg"},"source":["import torch\n","import transformers\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","from transformers import BertModel, BertTokenizer\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","%matplotlib inline\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVI3GeaQgh1F"},"source":["讀取先求有data_extract.py所處理完的資料，並將資料區分成train(49500筆)與validate(500筆)兩個部分"]},{"cell_type":"code","metadata":{"id":"95LwoBdNd_jk"},"source":["TRAIN = pd.read_json(\"./data/train.json\")\n","TRAIN = TRAIN.sample(frac=1).reset_index(drop=True)\n","VAL = pd.read_json(\"./data/test.json\")\n","VAL = VAL.sample(frac=1).reset_index(drop=True)\n","TRAIN = TRAIN.append(VAL[500:]).reset_index(drop=True)\n","VAL = VAL.iloc[:500]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GogBt1i7eNEw"},"source":["選擇所使用的育訓練模型中所搭配的分詞器(Tokenizer)"]},{"cell_type":"code","metadata":{"id":"IIjMqFNYd_jx"},"source":["PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","TOKENIZER = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nriIQtolebnB"},"source":["查看一般語句在BERT Tokenizer分詞過後的情形"]},{"cell_type":"code","metadata":{"id":"N2L1r52Hd_j0","outputId":"c995685b-0133-4c16-8d53-95df6815570a"},"source":["text = TRAIN.comment[0][:250]\n","\n","tokens = TOKENIZER.tokenize(text)\n","token_ids = TOKENIZER.convert_tokens_to_ids(tokens)\n","\n","print(tokens[:50])\n","print(token_ids[:50])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Titanic', 'directed', 'by', 'James', 'Cameron', 'presents', 'a', 'fictional', 'love', 'story', 'on', 'the', 'historical', 'setting', 'of', 'the', 'Titanic', '.', 'The', 'plot', 'is', 'simple', ',', 'non', '##com', '##plicate', '##d', ',', 'or', 'not', 'for', 'those', 'who', 'love', 'plots', 'that', 'twist', 'and', 'turn', 'and', 'keep', 'you', 'in', 'su', '##spense', '.', 'The', 'end', 'of', 'the']\n","[24342, 2002, 1118, 1600, 6681, 8218, 170, 6725, 1567, 1642, 1113, 1103, 3009, 3545, 1104, 1103, 24342, 119, 1109, 4928, 1110, 3014, 117, 1664, 8178, 21379, 1181, 117, 1137, 1136, 1111, 1343, 1150, 1567, 15836, 1115, 11079, 1105, 1885, 1105, 1712, 1128, 1107, 28117, 21643, 119, 1109, 1322, 1104, 1103]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h0635LD2eqYw"},"source":["差看各個特殊Token在BERT分詞器中的編碼"]},{"cell_type":"code","metadata":{"id":"FBUnHHMNd_j3","outputId":"7b5c08cd-efcd-4214-a4a3-91fac21be373"},"source":["print(TOKENIZER.sep_token, TOKENIZER.sep_token_id)\n","print(TOKENIZER.cls_token, TOKENIZER.cls_token_id)\n","print(TOKENIZER.pad_token, TOKENIZER.pad_token_id)\n","print(TOKENIZER.unk_token, TOKENIZER.unk_token_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[SEP] 102\n","[CLS] 101\n","[PAD] 0\n","[UNK] 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XwSYSlCyezm-"},"source":["依照資料及各筆語料分詞過後的長度選擇最大編碼長度(MAX_SEQ_LEN)"]},{"cell_type":"code","metadata":{"id":"5dIiUE_3d_j6","outputId":"e7804f17-d46b-4851-8a2e-69b071357127"},"source":["TRAIN[\"token_number\"] = TRAIN[\"comment\"].apply(TOKENIZER.tokenize).apply(len)\n","TRAIN[\"token_number\"].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    49500.000000\n","mean       317.248424\n","std        238.372051\n","min          8.000000\n","25%        169.000000\n","50%        236.000000\n","75%        386.000000\n","max       3238.000000\n","Name: token_number, dtype: float64"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"R_Xk0uG4d_j9"},"source":["MAX_SEQ_LEN = 160"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWht7SL8fEvJ"},"source":["BERT分詞器編碼的範例，其產生物件包含input_ids與attention_mask"]},{"cell_type":"code","metadata":{"id":"zGWG2DUgd_j_","outputId":"7f79418a-8f28-44f9-88a7-b6844e43482f"},"source":["encoding = TOKENIZER.encode_plus(\n","  text,\n","  max_length=MAX_SEQ_LEN,\n","  add_special_tokens=True,\n","  return_token_type_ids=False,\n","  pad_to_max_length=True,\n","  return_attention_mask=True,\n","  return_tensors='pt',\n",")\n","\n","print(encoding[\"input_ids\"][0])\n","print(encoding[\"attention_mask\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[  101, 24342,  2002,  1118,  1600,  6681,  8218,   170,  6725,  1567,\n","          1642,  1113,  1103,  3009,  3545,  1104,  1103, 24342,   119,  1109,\n","          4928,  1110,  3014,   117,  1664,  8178, 21379,  1181,   117,  1137,\n","          1136,  1111,  1343,  1150,  1567, 15836,  1115, 11079,  1105,  1885,\n","          1105,  1712,  1128,  1107, 28117, 21643,   119,  1109,  1322,  1104,\n","          1103,  2523,  1169,  1129,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RYEdsH6qgGpS"},"source":["查看編碼的原型: [CLS]...[SEP]...."]},{"cell_type":"code","metadata":{"id":"-HV_4M4Yd_kE","outputId":"51d9e891-547a-465c-bc7f-e0d511507626"},"source":["print(TOKENIZER.convert_ids_to_tokens(encoding[\"input_ids\"][0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]', 'Titanic', 'directed', 'by', 'James', 'Cameron', 'presents', 'a', 'fictional', 'love', 'story', 'on', 'the', 'historical', 'setting', 'of', 'the', 'Titanic', '.', 'The', 'plot', 'is', 'simple', ',', 'non', '##com', '##plicate', '##d', ',', 'or', 'not', 'for', 'those', 'who', 'love', 'plots', 'that', 'twist', 'and', 'turn', 'and', 'keep', 'you', 'in', 'su', '##spense', '.', 'The', 'end', 'of', 'the', 'movie', 'can', 'be', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"],"name":"stdout"}]}]}